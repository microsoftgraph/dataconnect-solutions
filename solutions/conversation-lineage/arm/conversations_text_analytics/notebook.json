{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2021-07-06T08:00:46.1496324Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# parameters\n",
        "sql_database_name = \"\"\n",
        "sql_table_name = \"\"\n",
        "sql_username = \"\"\n",
        "sql_password  = \"\"\n",
        "azure_ai_endpoint = \"\"\n",
        "azure_ai_key = \"\"\n",
        "sql_server_name = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "DefaultPool",
              "session_id": 43,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-06T08:00:46.2692928Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-06T08:00:46.3807866Z",
              "execution_finish_time": "2021-07-06T08:04:10.9418352Z"
            },
            "text/plain": "StatementMeta(DefaultPool, 43, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Reading conversations from SQL pool...\nAnalyzing conversations ...\nWriting conversation Sentimet into SQL pool ...\nWriting conversation Entities into SQL pool ...\nWriting conversation Recipient Sentiment into SQL pool ...\nCompleted"
        }
      ],
      "metadata": {},
      "source": [
        "import uuid\n",
        "import html2text\n",
        "import time\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "conversation_sentiment_info_sql_table_definition = \"id VARCHAR(1024), conversation_id VARCHAR(1024), sender_mail VARCHAR(1024), sender_name VARCHAR(1024), sender_domain VARCHAR(1024), \\\n",
        "                                                                                    general_sentiment VARCHAR(1024), pos_score FLOAT, neutral_score FLOAT, negative_score FLOAT\"\n",
        "conversation_entities_info_sql_table_definition = \"id VARCHAR(1024), conversation_id VARCHAR(1024), sender_mail VARCHAR(1024), sender_name VARCHAR(1024), sender_domain VARCHAR(1024), \\\n",
        "                                                                                    text VARCHAR(1024), category VARCHAR(1024), score FLOAT\"\n",
        "conversation_to_recipient_sentiment_info_sql_table_definition = \"id VARCHAR(1024), conversation_id VARCHAR(1024), sender_mail VARCHAR(1024), sender_name VARCHAR(1024), sender_domain VARCHAR(1024), \\\n",
        "                                                                                    general_sentiment VARCHAR(1024), pos_score FLOAT, neutral_score FLOAT, negative_score FLOAT, \\\n",
        "                                                                                    recipient_name VARCHAR(8000), recipient_address VARCHAR(8000), recipient_domain VARCHAR(8000)\"\n",
        "\n",
        "\n",
        "MAX_NUMBER_OF_RECIPIENTS = 50\n",
        "MAX_ENTITIES_RETRIEVED_FROM_MAIL = 10\n",
        "TEXT_ANALYTICS_BATCH_SIZE = 5\n",
        "\n",
        "def retrieve_conversations():\n",
        "    emails = spark.read.format(\"jdbc\") \\\n",
        "        .option(\"url\", f\"jdbc:sqlserver://{sql_server_name}.sql.azuresynapse.net:1433;database={sql_database_name};user={sql_username}@{sql_server_name};password={sql_password};encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\") \\\n",
        "        .option(\"user\", sql_username) \\\n",
        "        .option(\"password\", sql_password) \\\n",
        "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "        .option(\"query\", f\"SELECT Id, Content, Sender, Sender_Name, ToAddresses, ToNames from {sql_table_name}\") \\\n",
        "        .load()\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"Id\", StringType(), False),\n",
        "        StructField(\"Content\", StringType(), True),\n",
        "        StructField(\"Sender\", StringType(), True),\n",
        "        StructField(\"Sender_Name\", StringType(), True),\n",
        "        StructField(\"ToAddresses\", StringType(), True),\n",
        "        StructField(\"ToNames\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    emailsDf = spark.createDataFrame(emails.rdd, schema)\n",
        "\n",
        "    def transform(mail):\n",
        "        recipient_addresses_list = mail[\"ToAddresses\"].split(\",\")\n",
        "        recipient_names_list = mail[\"ToNames\"].split(\",\")\n",
        "        conv_id = mail['Id']\n",
        "        content = mail['Content']\n",
        "        sender_mail = mail[\"Sender\"]\n",
        "        sender_name = mail[\"Sender_Name\"]\n",
        "        sender_domain = sender_mail.split(\"@\")[1].strip()\n",
        "        recipient_addresses = recipient_addresses_list[:min(MAX_NUMBER_OF_RECIPIENTS, len(recipient_addresses_list))]\n",
        "        recipient_names = recipient_names_list[:min(MAX_NUMBER_OF_RECIPIENTS, len(recipient_names_list))]\n",
        "        recipient_domains = list(map(lambda mail: mail.split(\"@\")[1].strip(), recipient_addresses))\n",
        "\n",
        "        return {\n",
        "            \"sender_name\": sender_name,\n",
        "            \"id\": conv_id,\n",
        "            \"sender_mail\": sender_mail,\n",
        "            \"sender_domain\": sender_domain,\n",
        "            \"content\": content,\n",
        "            \"recipient_addresses\": recipient_addresses,\n",
        "            \"recipient_names\": recipient_names,\n",
        "            \"recipient_domains\": recipient_domains\n",
        "        }\n",
        "\n",
        "    conversarions = emailsDf.rdd.map(lambda x: transform(x))\n",
        "    return conversarions\n",
        "\n",
        "\n",
        "def analyze_conversations(all_conversations):\n",
        "    def analyze_per_partition(partition_data):\n",
        "        all_analyzed_conversations = []\n",
        "\n",
        "        text_analytics_client = TextAnalyticsClient(endpoint=azure_ai_endpoint, credential=AzureKeyCredential(azure_ai_key))\n",
        "        html_text_handler = html2text.HTML2Text()\n",
        "        html_text_handler.ignore_links = True\n",
        "        \n",
        "        def process_batch(batch):\n",
        "            print(\"Analyzing batch\")\n",
        "            conversations_sentiment_dict = dict()\n",
        "            conversation_entities_dict = dict()\n",
        "            try:\n",
        "                content = [html_text_handler.handle(conv['content']) for conv in batch]\n",
        "                #content = [html_text_handler.handle(conv['content'])]\n",
        "\n",
        "                entities_result = text_analytics_client.recognize_entities(content)\n",
        "                for entity_result in entities_result:\n",
        "                    index = entity_result[\"id\"]\n",
        "                    print(\"Entity_result keys:\",str(list(entity_result.keys())))\n",
        "                    if \"entities\" in list(entity_result.keys()):\n",
        "                        for recognized_ent in entity_result[\"entities\"]:\n",
        "                            category = recognized_ent[\"category\"]\n",
        "                            score = recognized_ent[\"confidence_score\"]\n",
        "                            text = recognized_ent[\"text\"]\n",
        "                            if score is not None and score > 0.5:\n",
        "                                ent_dict = dict(category=category, text=text, score=score)\n",
        "                                conversation_entities_dict.setdefault(index, []).append(ent_dict)\n",
        "                    elif \"error\" in entity_result:\n",
        "                        print(\"Error encountered\",str(entity_result))\n",
        "                    else:\n",
        "                        print(\"Error encountered\",str(entity_result))\n",
        "\n",
        "                sentiment_results = text_analytics_client.analyze_sentiment(content)\n",
        "                for sentiment_result in sentiment_results:\n",
        "                    index = sentiment_result[\"id\"]\n",
        "                    confidence_scores = sentiment_result[\"confidence_scores\"]\n",
        "                    general_sentiment = sentiment_result[\"sentiment\"]\n",
        "                    conversations_sentiment_dict.setdefault(index, [])\n",
        "                    sent_dict = dict(\n",
        "                        confidence_scores=confidence_scores,\n",
        "                        general_sentiment=general_sentiment,\n",
        "                        sentences=sentiment_result[\"sentences\"]\n",
        "                    )\n",
        "                    conversations_sentiment_dict[index] = sent_dict\n",
        "                    \n",
        "                \n",
        "                for idx, conversation in enumerate(batch):\n",
        "                    idx = str(idx)\n",
        "                    # we update the conversation only if we have information about it\n",
        "                    if idx in conversations_sentiment_dict and idx in conversation_entities_dict and len(conversations_sentiment_dict[idx]):\n",
        "                        conversation_sentiment_info = conversations_sentiment_dict[idx]\n",
        "                        conversation_entities_info = conversation_entities_dict[idx]\n",
        "\n",
        "                        conversation['conversation_sentiment_info'] = conversation_sentiment_info\n",
        "                        conversation['entities_info'] = conversation_entities_info\n",
        "                        all_analyzed_conversations.append(conversation)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(\"Exception in retrieving analyzed text. Error:\", e)\n",
        "\n",
        "        \n",
        "        # split rdd in batches\n",
        "        batch = []\n",
        "        batch_index = 0\n",
        "        for conv in partition_data:\n",
        "            batch_index += 1\n",
        "            batch.append(conv)\n",
        "            if batch_index == TEXT_ANALYTICS_BATCH_SIZE:\n",
        "                process_batch(batch)\n",
        "                batch = []\n",
        "                batch_index = 0\n",
        "        \n",
        "        if len(batch) > 0:\n",
        "            process_batch(batch)\n",
        "            batch_index = 0\n",
        "            batch = []\n",
        "        \n",
        "        return iter(all_analyzed_conversations)\n",
        "\n",
        "    return all_conversations.mapPartitions(analyze_per_partition)\n",
        "\n",
        "\n",
        "\n",
        "def create_conversation_sentiment_info(analyzed_conversations):\n",
        "    def transform(conversation):\n",
        "        sql_idx = str(uuid.uuid4())\n",
        "        conversation_id = conversation['id']\n",
        "        return [sql_idx,\n",
        "                conversation_id,\n",
        "                conversation['sender_mail'],\n",
        "                conversation['sender_name'],\n",
        "                conversation['sender_domain'],\n",
        "                conversation['conversation_sentiment_info'][\"general_sentiment\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"positive\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"neutral\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"negative\"],\n",
        "                ]\n",
        "\n",
        "\n",
        "    return analyzed_conversations.map(transform).toDF(\"id,conversation_id,sender_mail,sender_name,sender_domain,general_sentiment,pos_score,neutral_score,negative_score\".split(\",\"))\n",
        "\n",
        "\n",
        "def create_conversation_entities_info(analyzed_conversations):\n",
        "    def transform(conversation):\n",
        "        sql_idx = str(uuid.uuid4())\n",
        "        conversation_id = conversation['id']\n",
        "        entities_info = conversation[\"entities_info\"]\n",
        "        result = []\n",
        "        for ei in entities_info[:min(MAX_ENTITIES_RETRIEVED_FROM_MAIL, len(entities_info))]:\n",
        "            result.append([sql_idx,\n",
        "                    conversation_id,\n",
        "                    conversation['sender_mail'],\n",
        "                    conversation['sender_name'],\n",
        "                    conversation['sender_domain'],\n",
        "                    ei[\"text\"],\n",
        "                    ei[\"category\"],\n",
        "                    ei[\"score\"]\n",
        "                    ])\n",
        "        return result\n",
        "\n",
        "    return analyzed_conversations.flatMap(transform).toDF(\"id,conversation_id,sender_mail,sender_name,sender_domain,text,category,score\".split(\",\"))\n",
        "\n",
        "\n",
        "def create_conversation_to_recipient_sentiment_info(analyzed_conversations):\n",
        "    def transform(conversation):\n",
        "        sql_idx = str(uuid.uuid4())\n",
        "        conversation_id = conversation['id']\n",
        "        recipient_names = \",\".join(conversation['recipient_names'])\n",
        "        recipient_addresses = \",\".join(conversation['recipient_addresses'])\n",
        "        recipient_domain = \",\".join(conversation['recipient_domains'])\n",
        "        return [sql_idx,\n",
        "                conversation_id,\n",
        "                conversation['sender_mail'],\n",
        "                conversation['sender_name'],\n",
        "                conversation['sender_domain'],\n",
        "                conversation['conversation_sentiment_info'][\"general_sentiment\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"positive\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"neutral\"],\n",
        "                conversation['conversation_sentiment_info'][\"confidence_scores\"][\"negative\"],\n",
        "                recipient_names,\n",
        "                recipient_addresses,\n",
        "                recipient_domain\n",
        "                ]\n",
        "    \n",
        "    return analyzed_conversations.map(transform).toDF(\"id,conversation_id,sender_mail,sender_name,sender_domain,general_sentiment,pos_score,neutral_score,negative_score,recipient_name,recipient_address,recipient_domain\".split(\",\"))\n",
        "\n",
        "\n",
        "def write_df_to_sql(df, table_name, table_schema):\n",
        "    df.write.mode(\"overwrite\") \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", f\"jdbc:sqlserver://{sql_server_name}.sql.azuresynapse.net:1433;database={sql_database_name};user={sql_username}@{sql_server_name};password={sql_password};encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\") \\\n",
        "    .option(\"dbtable\", table_name) \\\n",
        "    .option(\"user\", sql_username) \\\n",
        "    .option(\"password\", sql_password) \\\n",
        "    .option(\"createTableColumnTypes\", table_schema) \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .save()\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Reading conversations from SQL pool...')\n",
        "    all_conversations = retrieve_conversations()\n",
        "    print('Analyzing conversations ...')\n",
        "    analyzed_conversations = analyze_conversations(all_conversations)\n",
        "    \n",
        "    # print(analyzed_conversations.count())\n",
        "    print('Writing conversation Sentimet into SQL pool ...')\n",
        "    conversation_sentiment_info = create_conversation_sentiment_info(analyzed_conversations)\n",
        "    write_df_to_sql(conversation_sentiment_info, \"dbo.conversation_sentiment_info\", conversation_sentiment_info_sql_table_definition)\n",
        "\n",
        "    print('Writing conversation Entities into SQL pool ...')\n",
        "    conversation_entities_info = create_conversation_entities_info(analyzed_conversations)\n",
        "    write_df_to_sql(conversation_entities_info, \"dbo.conversation_entities_info\", conversation_entities_info_sql_table_definition)\n",
        "\n",
        "    print('Writing conversation Recipient Sentiment into SQL pool ...')\n",
        "    conversation_to_recipient_sentiment_info = create_conversation_to_recipient_sentiment_info(analyzed_conversations)\n",
        "    write_df_to_sql(conversation_to_recipient_sentiment_info, \"dbo.conversation_to_recipient_sentiment_info\", conversation_to_recipient_sentiment_info_sql_table_definition)\n",
        "    \n",
        "\n",
        "    print('Completed')\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        ""
      ]
    }
  ]
}