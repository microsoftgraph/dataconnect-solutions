{"cells":[{"cell_type":"markdown","source":["# 0. Set the default lakehouse for notebook to run from pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"30ad755d-1796-42ce-ab0c-5c4fa9593ba7"},{"cell_type":"code","source":["%%configure\n","{ \n","    \"defaultLakehouse\": { \n","        \"name\": {\n","                  \"parameterName\": \"lakehouseName\",\n","                  \"defaultValue\": \"defaultlakehousename\"\n","        }\n","    }\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"6b63dc75-ee33-41f2-80e7-7de8ddc8459c"},{"cell_type":"markdown","source":["# 1. Initialize Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"a29638ad-ca6b-467b-9ad1-1963f28d964e"},{"cell_type":"code","source":["import java.time.LocalDateTime\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.UUID\n","import java.text.SimpleDateFormat\n","import java.time.{LocalDate, LocalDateTime, Period}\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.Calendar\n","import java.sql.Timestamp\n","\n","val runId  = \"00000000-0000-0000-0000-000000000000\"\n","val workspaceId =  spark.conf.get(\"trident.workspace.id\")\n","val workspaceName =  \"LakeHouseTesting\"\n","val lakehouseId = spark.conf.get(\"trident.lakehouse.id\")\n","val lakehouseName =   \"IMAXDefault\"\n","val sitesStagingTableName = \"Sites_Staging\"\n","val sitesFinalTableName = \"Sites\"\n","val permissionsStagingTableName = \"Permissions_Staging\"\n","val permissionsFinalTableName = \"Permissions\"\n","val endTime  = \"2024-11-15T00:00:00Z\"\n","spark.conf.set(\"spark.sql.caseSensitive\", true)// Welcome to your new notebook\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"23b81c9a-ad2e-436a-8f6d-45f2beaf90d2"},{"cell_type":"markdown","source":["# 2. Checking Required Final Tables exists or not"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"9a3ded46-9bad-466d-b360-c6464f4941b8"},{"cell_type":"code","source":["val lakehouse  = mssparkutils.lakehouse.get(lakehouseName)\n","val lakehouseId  = lakehouse.id\n","val workspaceName = notebookutils.runtime.context(\"currentWorkspaceName\")\n","\n","val permissionsStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${permissionsStagingTableName}\"\n","val permissionsFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${permissionsFinalTableName}\"\n","val sitesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesStagingTableName}\"\n","val sitesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesFinalTableName}\"\n","\n","\n","//Need to attach a lake house before this\n","val tables = spark.catalog.listTables()\n","val siteTableCount = tables.filter(col(\"name\") === lit(sitesFinalTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n","val permissionsTableCount = tables.filter(col(\"name\") === lit(permissionsFinalTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n","val siteStagingTableCount = tables.filter(col(\"name\") === lit(sitesStagingTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n","val permissionsStagingTableCount = tables.filter(col(\"name\") === lit(permissionsStagingTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"ebebff97-e480-40d1-bbaf-a0dc3ff09946"},{"cell_type":"markdown","source":["# 3. Getting Snapshot dates from last successful extracts"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"635b3aa1-c65d-4b5c-8cca-dc4ca8783527"},{"cell_type":"code","source":["import org.apache.spark.sql.functions.{col, _}\n","import org.apache.spark.sql.types._\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val dtCurrentDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.S\")\n","val dtRequiredtDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n","var siteDataExists: Boolean = false\n","var permissionsDataExists: Boolean = false\n","\n","val siteSnapshotDate = {\n","    if (siteTableCount ==1) {\n","        val dfSites = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${sitesFinalTableName} \")\n","        val rowSites: Row = dfSites.select(\"SnapshotDate\").head(1)(0)\n","        if (rowSites.get(0) == null) \n","            endTime \n","        else  \n","        {\n","            siteDataExists = true\n","            println(s\"Sites data Exists: ${siteDataExists}\")\n","            LocalDateTime.parse(rowSites.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt)\n","        }\n","    }\n","    else {\n","        endTime\n","    }\n","}\n","\n","val permissionsSnapshotDate = {\n","    if (permissionsTableCount ==1) {\n","        val dfPermissions = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${permissionsFinalTableName} \")\n","        val rowPermissions: Row = dfPermissions.select(\"SnapshotDate\").head(1)(0)\n","        if (rowPermissions.get(0) == null) \n","            endTime \n","        else {\n","            permissionsDataExists = true\n","            println(s\"Permissions data Exists: ${permissionsDataExists}\")\n","            LocalDateTime.parse(rowPermissions.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt) \n","        }  \n","    }\n","    else {\n","        endTime\n","    }\n","}\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"b8ee1aaa-4fce-409f-b624-3c8055c5c134"},{"cell_type":"markdown","source":["# 4. Generate View Script for Sites"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"84ce54e4-2dbf-4940-b23d-49f8fdad0d7d"},{"cell_type":"code","source":["val sitesView: String = s\"\"\"\n","CREATE OR ALTER VIEW vw${sitesFinalTableName}   \n","AS\n","SELECT  *,[StorageQuotaFriendly] =  (case \n","                when StorageQuota < 1048576 then concat(ceiling(StorageQuota / 1024.0), ' KB')\n","                when StorageQuota < 1073741824 then concat(ceiling(StorageQuota / 1048576.0), ' MB')\n","                when StorageQuota < 1099511627776  then concat(ceiling(StorageQuota / 1073741824.0), ' GB')\n","                when StorageQuota < 1125899906842624  then concat(ceiling(StorageQuota / 1099511627776.0), ' TB')\n","                else concat(ceiling(StorageQuota / 1125899906842624.0), ' PB')\n","            end )\n","        ,[StorageUsedFriendly] =  (case \n","                when StorageUsed < 1048576 then concat(ceiling(StorageUsed / 1024.0), ' KB')\n","                when StorageUsed < 1073741824 then concat(ceiling(StorageUsed / 1048576.0), ' MB')\n","                when StorageUsed < 1099511627776  then concat(ceiling(StorageUsed / 1073741824.0), ' GB')\n","                when StorageUsed < 1125899906842624  then concat(ceiling(StorageUsed / 1099511627776.0), ' TB')\n","                else concat(ceiling(StorageUsed / 1125899906842624.0), ' PB')\n","            end )            \n","  FROM ${sitesFinalTableName}\n","\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n","println(sitesView)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"c3ac128e-e185-4345-846a-d59e53bd8c0c"},{"cell_type":"markdown","source":["# 5. Generate View Script for Permissions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"0a99d604-0fe2-448c-bb65-e9f7570347c8"},{"cell_type":"code","source":["val permissionsView: String = s\"\"\"\n","CREATE OR ALTER VIEW vw${permissionsFinalTableName}         \n","    AS      \n","SELECT *,\n","ShareeDomain = CASE WHEN CHARINDEX('@', SharedWith_Email) > 0 AND CHARINDEX('.', SharedWith_Email) > 0 THEN SUBSTRING(SharedWith_Email,CHARINDEX('@', SharedWith_Email)+1,LEN(SharedWith_Email)) ELSE '' END,\n","ShareeEMail = CASE WHEN CHARINDEX('@', SharedWith_Email) > 0 THEN SharedWith_Email ELSE '' END,\n","PermissionsUniqueKey = CONCAT(SiteId,'_',RoleDefinition,'_',ScopeId,'_',COALESCE(LinkId,'00000000-0000-0000-0000-000000000000')),\n","EEEUPermissionsCount = SUM(CASE WHEN SharedWith_Name LIKE 'Everyone except external users' THEN 1 ELSE NULL END ) OVER(PARTITION BY CONCAT(SiteId,'_',RoleDefinition,'_',ScopeId,'_',COALESCE(LinkId,'00000000-0000-0000-0000-000000000000'),SharedWith_Name)),\n","ExternalUserCount = SUM(CASE WHEN SharedWith_TypeV2 LIKE 'External' THEN 1 ELSE NULL END ) OVER(PARTITION BY CONCAT(SiteId,'_',RoleDefinition,'_',ScopeId,'_',COALESCE(LinkId,'00000000-0000-0000-0000-000000000000'),SharedWith_Name)),\n","B2BUserCount = SUM(CASE WHEN SharedWith_TypeV2 LIKE 'B2BUser' THEN 1 ELSE NULL END ) OVER(PARTITION BY CONCAT(SiteId,'_',RoleDefinition,'_',ScopeId,'_',COALESCE(LinkId,'00000000-0000-0000-0000-000000000000'),SharedWith_Name))\n","FROM ${permissionsFinalTableName}\n","\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n","println(permissionsView)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"57f35d38-9f13-4d08-b93c-73b79c13f320"},{"cell_type":"markdown","source":["# 6. Truncate the Staging tables from previous runs if data already exists"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"ea1e5bdb-4999-47d2-93b5-4c101737d2cd"},{"cell_type":"code","source":["if (siteStagingTableCount ==1) {\n","    spark.sql(s\"DELETE FROM ${lakehouseName}.${sitesStagingTableName} \")\n","    println(s\"Staging table deleted: ${lakehouseName}.${sitesStagingTableName}\")\n","}else {\n","    println(s\"Staging table ${lakehouseName}.${sitesFinalTableName} not found\")\n","}\n","\n","\n","if (permissionsStagingTableCount ==1) {\n","    spark.sql(s\"DELETE FROM ${lakehouseName}.${permissionsStagingTableName} \")\n","    println(s\"Staging table deleted: ${lakehouseName}.${permissionsStagingTableName}\")\n","}else {\n","    println(s\"Staging table ${lakehouseName}.${permissionsStagingTableName} not found\")\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"d489c12b-ecf3-4ec1-af7e-2509246fd4e0"},{"cell_type":"markdown","source":["# 7. Return snapshot dates back to Pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"70fb943c-211e-468f-9c6c-f8319720197d"},{"cell_type":"code","source":["import mssparkutils.notebook\n","val returnData= s\"\"\"{\\\"LakehouseId\\\": \\\"${lakehouseId}\\\", \\\"SitesStagingTableName\\\": \\\"${sitesStagingTableName}\\\", \\\"SitesFinalTableName\\\": \\\"${sitesFinalTableName}\\\",  \\\"SitesSnapshotDate\\\": \\\"${siteSnapshotDate}\\\", \\\"SitesDataExists\\\": ${siteDataExists}, \\\"SitesView\\\": \\\"${sitesView}\\\",  \\\"PermissionsStagingTableName\\\": \\\"${permissionsStagingTableName}\\\", \\\"PermissionsFinalTableName\\\": \\\"${permissionsFinalTableName}\\\", \\\"PermissionsSnapshotDate\\\": \\\"${permissionsSnapshotDate}\\\", \\\"EndSnapshotDate\\\": \\\"${endTime}\\\", \\\"PermissionsDataExists\\\": ${permissionsDataExists}, \\\"PermissionsView\\\": \\\"${permissionsView}\\\"}\"\"\"\n","println(returnData)\n","mssparkutils.notebook.exit(returnData)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"cf9c0f98-b0d0-40f5-ac5b-8d92f86e5656"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"scala"},"microsoft":{"language":"scala","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}