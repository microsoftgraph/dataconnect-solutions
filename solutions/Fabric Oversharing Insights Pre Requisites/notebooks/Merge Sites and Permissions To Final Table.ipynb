{"cells":[{"cell_type":"markdown","source":["# 0. Set the default lakehouse for notebook to run"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"506d9ad0-1561-4656-a4a6-ce8d34633858"},{"cell_type":"code","source":["%%configure\n","{ \n","    \"defaultLakehouse\": { \n","        \"name\": {\n","                  \"parameterName\": \"lakehouseName\",\n","                  \"defaultValue\": \"defaultlakehousename\"\n","        }\n","    }\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"6584ead2-8df1-4d14-b610-b151e0ad37e9"},{"cell_type":"markdown","source":["# 1. Initialize Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ab7f1ea0-3bb6-409d-859e-684890193df1"},{"cell_type":"code","source":["import java.time.LocalDateTime\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.UUID\n","import java.text.SimpleDateFormat\n","import java.time.{LocalDate, LocalDateTime, Period}\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.Calendar\n","\n","val runId  = \"00000000-0000-0000-0000-000000000000\"\n","val workspaceId =  spark.conf.get(\"trident.workspace.id\")\n","val workspaceName =  \"LakeHouseTesting\"\n","val lakehouseId = spark.conf.get(\"trident.lakehouse.id\")\n","val lakehouseName = spark.conf.get(\"trident.lakehouse.name\")\n","val sitesStagingTableName = \"Sites_Staging\"\n","val sitesFinalTableName = \"Sites\"\n","val permissionsStagingTableName = \"Permissions_Staging\"\n","val permissionsFinalTableName = \"Permissions\"\n","spark.conf.set(\"spark.sql.caseSensitive\", true)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"a677d4e1-9d70-4143-91b6-65baef85e601"},{"cell_type":"markdown","source":["\n","# 2. Read Sites Dataset from Staging Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"449fb903-cb8e-4fe5-93e2-97870f358081"},{"cell_type":"code","source":["val lakehouse  = mssparkutils.lakehouse.get(lakehouseName)\n","val lakehouseId  = lakehouse.id\n","val workspaceName = notebookutils.runtime.context(\"currentWorkspaceName\")\n","println(\"Started reading Sites dataset\")\n","val sitesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesStagingTableName}\"\n","val dfSitesStaging = spark.read.format(\"delta\").load(sitesStagingLocation)\n","println(\"Completed reading Sites dataset\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"9801b8e1-de24-48eb-91a2-1ca1e659846e"},{"cell_type":"markdown","source":["# 3. Read Permissions Dataset from Staging Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"e3d2123b-7b48-4d9d-b2b5-3be47a1e6f78"},{"cell_type":"code","source":["println(\"Started reading Permissions dataset\")\n","val permissionsStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${permissionsStagingTableName}\"\n","val dfPermissionsStaging = spark.read.format(\"delta\").load(permissionsStagingLocation)\n","println(\"Completed reading Permissions dataset\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"7caee3ce-c21e-4a4b-abc8-f496a0ef0b35"},{"cell_type":"markdown","source":["# 4. Check Final Tables Exists or not "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"eeab8dde-3ee6-4bd2-9ab7-7cb0b4703513"},{"cell_type":"code","source":["import io.delta.tables.DeltaTable\n","val sitesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesFinalTableName}\"\n","val permissionsFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${permissionsFinalTableName}\"\n","\n","\n","val sitesFinalTableExists = DeltaTable.isDeltaTable(spark, sitesFinalLocation)\n","if (!sitesFinalTableExists) {\n","    println(\"Final Sites table not exists. Creating final Sites table with schema only\")\n","    dfSitesStaging.filter(\"1=2\").write.format(\"delta\").mode(\"overwrite\").save(sitesFinalLocation)\n","    println(\"Final Sites table created\")\n","}else {\n","    println(\"Final Sites table exists already\")\n","}\n","\n","\n","\n","val permissionsFinalTableExists = DeltaTable.isDeltaTable(spark, permissionsFinalLocation)\n","if (!permissionsFinalTableExists) {\n","    println(\"Final Permissions table not exists. Creating final Permissions table with schema only\")\n","    dfPermissionsStaging.filter(\"1=2\").write.format(\"delta\").mode(\"overwrite\").save(permissionsFinalLocation)\n","    println(\"Final Permissions table created\")\n","}else {\n","    println(\"Final Permissions table exists already\")\n","}\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"dfd80dfb-017b-42c4-aec3-9abe1ba84b44"},{"cell_type":"markdown","source":["# 5. Merge Sites Data from Staging table to Final table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"a7e5dea4-90f3-4029-af65-8cef8eca910e"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val deltaTableSource = DeltaTable.forPath(spark, sitesStagingLocation)\n","val deltaTableTarget = DeltaTable.forPath(spark, sitesFinalLocation)\n","\n","import spark.implicits._\n","val dfSource = deltaTableSource.toDF\n","\n","//Delete records that have Operation as Deleted \n","println(\"Merging Sites dataset from current staging table\")\n","deltaTableTarget\n","  .as(\"target\")\n","  .merge(\n","    dfSource.as(\"source\"),\n","    \"source.Id = target.Id\")\n","  .whenMatched(\"source.Operation = 'Deleted'\")\n","  .delete()\n","  .whenMatched(\"source.Operation != 'Deleted'\")\n","  .updateAll()\n","  .whenNotMatched(\"source.Operation != 'Deleted'\")\n","  .insertAll()\n","  .execute()\n","println(\"Merging of Sites dataset completed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"117dc88d-f2d8-42de-82b2-d61bca2bc20e"},{"cell_type":"markdown","source":["# 6. Merge Permissions Data from Staging table to Final table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"2f67773d-edd3-4de2-8acf-48b9a0ce52e3"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val deltaTablePermissionsSource = DeltaTable.forPath(spark, permissionsStagingLocation)\n","val deltaTablePermissionsTarget = DeltaTable.forPath(spark, permissionsFinalLocation)\n","\n","import spark.implicits._\n","val dfPermissionsSource = deltaTablePermissionsSource.toDF\n","\n","//Delete records that have Operation as Deleted \n","println(\"Merging Permissions dataset from current staging table\")\n","\n","//Step 1: Delete all rows based on keys\n","\n","println(\"Started: Cleaned up older permissions\")\n","deltaTablePermissionsTarget\n","  .as(\"target\")\n","  .merge(\n","    dfPermissionsSource.as(\"source\"),\n","    \"\"\"source.SiteId = target.SiteId and source.ScopeId = target.ScopeId and coalesce(source.LinkId,'') = coalesce(target.LinkId,'') and source.RoleDefinition = target.RoleDefinition \"\"\")\n","  .whenMatched()\n","  .delete()\n","  .execute()\n","println(\"Complted: Cleaned up older permissions\")\n","\n","deltaTablePermissionsTarget\n","  .as(\"target\")\n","  .merge(\n","    dfPermissionsSource.as(\"source\"),\n","    \"\"\"source.SiteId = target.SiteId and source.ScopeId = target.ScopeId and coalesce(source.LinkId,'') = coalesce(target.LinkId,'') and source.RoleDefinition = target.RoleDefinition and \n","     coalesce(source.SharedWith_Name,\"\") = coalesce(target.SharedWith_Name,\"\") and coalesce(source.SharedWith_TypeV2,\"\") = coalesce(target.SharedWith_TypeV2,\"\") and \n","     coalesce(source.SharedWith_Email,\"\") = coalesce(target.SharedWith_Email,\"\") and coalesce(source.SharedWith_AADObjectId,\"\") = coalesce(target.SharedWith_AADObjectId,\"\") \"\"\")\n","  .whenMatched(\"source.Operation = 'Deleted'\")\n","  .delete()\n","  .whenMatched(\"source.Operation != 'Deleted'\")\n","  .updateAll()\n","  .whenNotMatched(\"source.Operation != 'Deleted'\")\n","  .insertAll()\n","  .execute()\n","  println(\"Merging of Permissions dataset completed\")\n","  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"c77d8298-74b4-4807-b345-a797b9ef5892"},{"cell_type":"markdown","source":["# 7. Read Sites and Permissions dataset - Sample TOP 10 Rows"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"b19806ee-f1ff-49e5-a0e6-32daa089f2bd"},{"cell_type":"code","source":["var sqlQuery = s\"SELECT * FROM ${lakehouseName}.${sitesFinalTableName} order by SnapshotDate DESC LIMIT 10\"\n","val dfSitesAll = spark.sql(sqlQuery)\n","display(dfSitesAll)\n","\n","sqlQuery = s\"SELECT * FROM ${lakehouseName}.${permissionsFinalTableName}  order by SnapshotDate DESC LIMIT 10\"\n","val dfPermissionsAll = spark.sql(sqlQuery)\n","display(dfPermissionsAll)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"72c368c1-a1c1-410e-8f81-c2201f87a148"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"scala"},"microsoft":{"language":"scala","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}