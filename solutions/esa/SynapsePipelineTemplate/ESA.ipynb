{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATIONS\r\n",
        "\r\n",
        "# Inputs\r\n",
        "emailPath = \"abfss://mgdc@esastore7wypqfcc6p3wk.dfs.core.windows.net/email_2022-06-01_to_2022-07-01/\"\r\n",
        "teamsChatPath = \"abfss://mgdc@esastore7wypqfcc6p3wk.dfs.core.windows.net/teamschat_2022-06-01_to_2022-07-01/\"\r\n",
        "\r\n",
        "# Limit (for performance / throttling)\r\n",
        "limit = 10000\r\n",
        "\r\n",
        "# Date Format\r\n",
        "dtformat = '%Y-%m-%dT%H:%M:%S.%f' # <<< for email and teamschat datasets\r\n",
        "\r\n",
        "#Output Format: Can be csv or parquet\r\n",
        "#outputFormat = \"csv\"\r\n",
        "outputFormat = \"parquet\"\r\n",
        "\r\n",
        "# Output Paths\r\n",
        "outputPath = \"abfss://output@esastore7wypqfcc6p3wk.dfs.core.windows.net/entity_sentiment_analysis.csv\"\r\n",
        "\r\n",
        "# StartDate/EndDate for this run that is denormalized to users and interactions tables\r\n",
        "period = \"2022-06-01 to 2022-07-01\"\r\n",
        "\r\n",
        "# Whether or not to md5 hash the input user emails\r\n",
        "obfuscateEmails = True\r\n",
        "\r\n",
        "# Whether the input MGDC data is parquet (True) or json (False)\r\n",
        "isParquetInput = False\r\n",
        "\r\n",
        "# Leiden max cluster size, the maximum possible size for a detected community\r\n",
        "leidenMaxClusterSize = 1000\r\n",
        "\r\n",
        "# The Ignore List, if any of the data wishes to be filtered for certain entities\r\n",
        "ignore_list = [\r\n",
        "    'Secret Product',\r\n",
        "    'Secret Sauce'\r\n",
        "]\r\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "\r\n",
        "from pyspark.sql.functions import coalesce, col, count, explode, lit, md5, size, udf, max, countDistinct, dense_rank, monotonically_increasing_id\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.sql.functions import array, when\r\n",
        "from pyspark.sql.types import ArrayType, StringType, StructField, StructType, IntegerType, FloatType, DoubleType, LongType\r\n",
        "from pyspark.sql import types as t\r\n",
        "from pyspark.sql import SparkSession, Row\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "\r\n",
        "from requests import Request\r\n",
        "\r\n",
        "from synapse.ml.cognitive import *\r\n",
        "from synapse.ml import *\r\n",
        "from synapse.ml.core.platform import *\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "import re\r\n",
        "import os\r\n",
        "import typing"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONNECTION TO Azure Cognitive Services\r\n",
        "\r\n",
        "# A general Cognitive Services key for Text Analytics\r\n",
        "key = \"775eae09abb94d8a983c1f72d1caaab2\"\r\n",
        "loc = \"eastus2\"\r\n",
        "endpoint = \"https://acslanguageresource7wypqfcc6p3wk.cognitiveservices.azure.com/\"\r\n",
        "\r\n",
        "# Authenticate the client using key and endpoint \r\n",
        "def authenticate_client():\r\n",
        "    ta_credential = AzureKeyCredential(key)\r\n",
        "    text_analytics_client = TextAnalyticsClient(\r\n",
        "            endpoint=endpoint, \r\n",
        "            credential=ta_credential)\r\n",
        "    return text_analytics_client\r\n",
        "\r\n",
        "client = authenticate_client()"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA\r\n",
        "\r\n",
        "areEmailsLoaded = False\r\n",
        "areTeamsChatsLoaded = False\r\n",
        "\r\n",
        "# EMAILS\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        emailsRaw = spark.read.parquet(emailPath).select(\"Id\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        emailsRaw = spark.read.json(emailPath)\r\n",
        "        \r\n",
        "        \r\n",
        "    areEmailsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Emails data not loaded, continuing with empty emails\")\r\n",
        "    emailsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    emailsRaw = spark.createDataFrame(sc.emptyRDD(), emailsSchema)\r\n",
        "\r\n",
        "\r\n",
        "# TEAMS CHATS\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        teamschatsRaw = spark.read.parquet(teamsChatPath).select(\"Id\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        teamschatsRaw = spark.read.json(teamsChatPath)\r\n",
        "    areTeamsChatsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"TeamsChats data not loaded, continuing with empty teams chats\")\r\n",
        "    teamschatsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    teamschatsRaw = spark.createDataFrame(sc.emptyRDD(), teamschatsSchema)\r\n",
        "\r\n",
        "\r\n",
        "if (not(areEmailsLoaded) and not(areTeamsChatsLoaded)):\r\n",
        "    raise Exception(\"No Emails or TeamsChats data loaded, unable to continue. Check the file paths.\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMAIL CLEANING AND PREPPING\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.select(\"createdDateTime\", \"sender\", \"uniqueBody\")\r\n",
        "emailsRaw = emailsRaw.dropDuplicates()\r\n",
        "emailsRaw = emailsRaw.withColumnRenamed(\"uniqueBody\", \"body\")\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.limit(limit)\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.withColumn(\"createdDateTime\", emailsRaw[\"createdDateTime\"].cast(StringType()))\r\n",
        "emailsRaw = emailsRaw.withColumn(\"sender\", emailsRaw[\"sender\"].cast(StringType()))\r\n",
        "emailsRaw = emailsRaw.withColumn(\"body\", emailsRaw[\"body\"].cast(StringType()))\r\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEAMS CHATS CLEANING AND PREPPING\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.select(\"CreatedDateTime\", \"Sender\", \"Body\")\r\n",
        "teamschatsRaw = teamschatsRaw.dropDuplicates()\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"CreatedDateTime\", teamschatsRaw[\"CreatedDateTime\"].cast(StringType()))\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"Sender\", teamschatsRaw[\"Sender\"].cast(StringType()))\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"Body\", teamschatsRaw[\"Body\"].cast(StringType()))\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"CreatedDateTime\", \"createdDateTime\")\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"Sender\", \"sender\")\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"Body\", \"body\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMAIL PARSER (UNIQUE BODY)\r\n",
        "\r\n",
        "# given raw html email body (unique body), returns only text\r\n",
        "def parse_email(raw_email):\r\n",
        "\r\n",
        "    message = \"\"\r\n",
        "\r\n",
        "    bracket = False\r\n",
        "    for char in raw_email: \r\n",
        "        if (char == \"<\"):\r\n",
        "            bracket = False\r\n",
        "\r\n",
        "        if (bracket):\r\n",
        "            message = message + char\r\n",
        "\r\n",
        "        if (char == \">\"):\r\n",
        "            bracket = True\r\n",
        "\r\n",
        "    message_with_HTML_removed = \" \".join(word for word in message.split() if \"HTML\" not in word)\r\n",
        "\r\n",
        "    return message_with_HTML_removed\r\n",
        "\r\n",
        "\r\n",
        "# ***used in conjunction with a spark map function\r\n",
        "# given a row in an email dataframe, returns the row with the body converted to only text\r\n",
        "def parse_bodies_html_to_text(row):\r\n",
        "        raw_email = row[\"body\"]\r\n",
        "\r\n",
        "        body = parse_email(raw_email)\r\n",
        "\r\n",
        "        row_dict = row.asDict()\r\n",
        "\r\n",
        "        row_dict.update({\"body\" : body})\r\n",
        "\r\n",
        "        row = Row(**row_dict)\r\n",
        "\r\n",
        "        return row\r\n",
        "    \r\n",
        "emails = emailsRaw.rdd.map(parse_bodies_html_to_text).toDF()\r\n",
        "teamschats = teamschatsRaw.rdd.map(parse_bodies_html_to_text).toDF()"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MERGE EMAIL AND TEAMS CHATS DATAFRAMES\r\n",
        "\r\n",
        "emails_and_teamschats = emails.union(teamschats)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATE PARSER\r\n",
        "\r\n",
        "def parse_date(date_string, date_format):\r\n",
        "    try:\r\n",
        "        date_string = date_string.strip()\r\n",
        "\r\n",
        "        date_obj = datetime.strptime(date_string, date_format)\r\n",
        "    \r\n",
        "        return date_obj.strftime('%Y-%m-%d')\r\n",
        "    except ValueError:\r\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MINE OPINIONS\r\n",
        "\r\n",
        "def mine_opinions(client, documents):\r\n",
        "\r\n",
        "    # limit all documents to the first 5000 text elements to prevent throttling\r\n",
        "    documents = [doc[:5000] for doc in documents]\r\n",
        "\r\n",
        "    sentiment_result = client.analyze_sentiment(documents, show_opinion_mining=True)\r\n",
        "    entity_result = client.recognize_entities(documents)\r\n",
        "\r\n",
        "    entity_sentiment_tuples = []\r\n",
        "\r\n",
        "    document = 0\r\n",
        "    while (document < len(documents)):\r\n",
        "\r\n",
        "        categories_to_remove = [\"Person\", \"Address\", \"PhoneNumber\", \"Email\", \"URL\", \"IP\", \"DateTime\", \"Quantity\"]\r\n",
        "\r\n",
        "        document_tuples = [(CategorizedEntity, sentiment_result[document].sentiment) for CategorizedEntity in entity_result[document].entities if CategorizedEntity.category not in categories_to_remove]\r\n",
        "\r\n",
        "        entities = []\r\n",
        "        for tup in document_tuples:\r\n",
        "            if (tup[0].confidence_score > 0.6):\r\n",
        "                entity = tup[0].text\r\n",
        "                entity = re.sub(r'[^\\w\\s]', '', entity)\r\n",
        "                entity = entity.title()\r\n",
        "                entities.append((entity, tup[1]))\r\n",
        "\r\n",
        "        # fixes an issue where entities are duplicated based on their subcategories\r\n",
        "        entities_dedup = []\r\n",
        "        [entities_dedup.append(tup) for tup in entities if tup not in entities_dedup]\r\n",
        "\r\n",
        "        # filters out any entities included on the ignore list\r\n",
        "        entities = [tup for tup in entities_dedup if tup[0] not in ignore_list]\r\n",
        "\r\n",
        "        for sentence in sentiment_result[document].sentences: # break each document into sentences\r\n",
        "\r\n",
        "            if (sentence.mined_opinions): # if sentence.mined_opinions != 0 \r\n",
        "                for mined_opinion in sentence.mined_opinions: # go through mined_opinions            \r\n",
        "                    target = mined_opinion.target \r\n",
        "\r\n",
        "                    i = 0\r\n",
        "                    while (i < len(entities)):\r\n",
        "                        if (target.text == entities[i][0]):\r\n",
        "                            entities[i] = (target.text, target.sentiment)\r\n",
        "                        i = i + 1\r\n",
        "\r\n",
        "        entity_sentiment_tuples.append(entities)\r\n",
        "\r\n",
        "        document = document + 1\r\n",
        "\r\n",
        "    return entity_sentiment_tuples"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATAFRAME CREATION\r\n",
        "\r\n",
        "def add_entities_and_sentiment_columns_to_dataframe(df):\r\n",
        "\r\n",
        "    df = df.withColumn(\"entities_and_sentiments\", lit(\"\"))\r\n",
        "\r\n",
        "    distinct_sender = df.select(\"sender\").distinct().orderBy(\"sender\")\r\n",
        "    sender_numbers = distinct_sender.withColumn(\"sender_id\", monotonically_increasing_id())\r\n",
        "    df = df.join(sender_numbers, \"sender\")\r\n",
        "\r\n",
        "    df = df.repartition(int(limit / 5))\r\n",
        "\r\n",
        "    def gather_entities_and_sentiments(client, rows):\r\n",
        "        copy_rows = list(rows)\r\n",
        "\r\n",
        "        list_of_lists_of_entity_sentiment_tuples = mine_opinions(client, [row[\"body\"] for row in copy_rows])\r\n",
        "\r\n",
        "        updated_rows = []\r\n",
        "        i = 0\r\n",
        "        for row in copy_rows: \r\n",
        "            updated_row = Row(sender = row.sender_id, body = row.body, createdDateTime = row.createdDateTime, entities_and_sentiments = list_of_lists_of_entity_sentiment_tuples[i])\r\n",
        "            updated_rows.append(updated_row)\r\n",
        "            i = i + 1\r\n",
        "\r\n",
        "        return updated_rows\r\n",
        "\r\n",
        "\r\n",
        "    df = df.rdd.mapPartitions(lambda rows: gather_entities_and_sentiments(client, rows)).toDF()\r\n",
        "\r\n",
        "    df = df.filter(col(\"entities_and_sentiments\") != array([])) \r\n",
        "    df = df.withColumnRenamed(\"sender\", \"Sender\")\r\n",
        "    df = df.select(\"Sender\", \"createdDateTime\", explode(\"entities_and_sentiments\").alias(\"entity_and_sentiment\"))\r\n",
        "\r\n",
        "    df = df.withColumn(\"Topic\", col(\"entity_and_sentiment\")[\"_1\"])\r\n",
        "    df = df.withColumn(\"Sentiment\", col(\"entity_and_sentiment\")[\"_2\"])\r\n",
        "\r\n",
        "    df = df.drop(\"entity_and_sentiment\")\r\n",
        "\r\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT DATAFRAME\r\n",
        "\r\n",
        "def create_output_dataframe_with_esa_schematics(df):\r\n",
        "    \r\n",
        "    df = df.withColumn(\"Mentions\", lit(1))\r\n",
        "\r\n",
        "    df = df.withColumn(\"Positive\", when(col(\"sentiment\") == \"positive\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Neutral\", when(col(\"sentiment\") == \"neutral\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Mixed\", when(col(\"sentiment\") == \"mixed\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Negative\", when(col(\"sentiment\") == \"negative\", 1).otherwise(0))\r\n",
        "\r\n",
        "    df = df.drop(\"Sentiment\")\r\n",
        "\r\n",
        "    udf_parse_date = udf(parse_date, StringType())\r\n",
        "    df = df.withColumn(\"Date Format\", lit(dtformat))\r\n",
        "    df = df.withColumn(\"Date\", udf_parse_date(col(\"createdDateTime\"), col(\"Date Format\")))\r\n",
        "    df = df.drop(\"createdDateTime\").drop(\"Date Format\")\r\n",
        "\r\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT CSV\r\n",
        "\r\n",
        "print(\"Getting input dataframe...\")\r\n",
        "df = emails_and_teamschats\r\n",
        "\r\n",
        "print(\"Finding entities and sentiments...\")\r\n",
        "df = add_entities_and_sentiment_columns_to_dataframe(df)\r\n",
        "\r\n",
        "print(\"Creating output dataframe...\")\r\n",
        "df = create_output_dataframe_with_esa_schematics(df)\r\n",
        "\r\n",
        "if outputFormat == \"csv\":\r\n",
        "    print(\"Coalescing dataframe...\")\r\n",
        "    df.coalesce(1)\r\n",
        "    print(\"Saving DataFrame as CSV...\")\r\n",
        "    df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(outputPath)\r\n",
        "\r\n",
        "elif outputFormat == \"parquet\":\r\n",
        "    print(\"Coalescing dataframe...\")\r\n",
        "    df.coalesce(1)\r\n",
        "    print(\"Saving DataFrame as Parquet...\") \r\n",
        "    df.write.option(\"header\", True).mode(\"overwrite\").parquet(outputPath)\r\n",
        "\r\n",
        "print(\"Done!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}