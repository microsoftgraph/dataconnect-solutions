{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATIONS\r\n",
        "\r\n",
        "# Inputs\r\n",
        "emailPath = \"abfss://mgdc@your_esa_store.dfs.core.windows.net/email_2022-06-01_to_2022-07-01/\"\r\n",
        "teamsChatPath = \"abfss://mgdc@your_esa_store.dfs.core.windows.net/teamschat_2022-06-01_to_2022-07-01/\"\r\n",
        "enronEmailPath = \"abfss://demodata@your_esa_store.dfs.core.windows.net/enronEmails.json\"\r\n",
        "twitterPostPath = \"abfss://demodata@your_esa_store.dfs.core.windows.net/TweetData.json\"\r\n",
        "\r\n",
        "# Limit (for performance / throttling)\r\n",
        "limit = 10000\r\n",
        "\r\n",
        "# Date Format\r\n",
        "dtformat = '%Y-%m-%dT%H:%M:%S.%f' # <<< for email and teamschat datasets\r\n",
        "#dtformat = '%a, %d %b %Y %H:%M:%S %z' # <<< for Enron dataset\r\n",
        "#dtformat = \"%Y-%m-%d %H:%M:%S %z\" # <<< for twitter posts\r\n",
        "\r\n",
        "#Output Format: Can be csv or parquet\r\n",
        "#outputFormat = \"csv\"\r\n",
        "outputFormat = \"parquet\"\r\n",
        "\r\n",
        "# Output Paths\r\n",
        "outputPath = \"abfss://output@your_esa_store.dfs.core.windows.net/entity_sentiment_analysis.csv\"\r\n",
        "\r\n",
        "# StartDate/EndDate for this run that is denormalized to users and interactions tables\r\n",
        "period = \"2022-06-01 to 2022-07-01\"\r\n",
        "\r\n",
        "# Whether or not to md5 hash the input user emails\r\n",
        "obfuscateEmails = True\r\n",
        "\r\n",
        "# Whether the input MGDC data is parquet (True) or json (False)\r\n",
        "isParquetInput = False\r\n",
        "\r\n",
        "# Leiden max cluster size, the maximum possible size for a detected community\r\n",
        "leidenMaxClusterSize = 1000\r\n",
        "\r\n",
        "# The Ignore List, if any of the data wishes to be filtered for certain entities\r\n",
        "ignore_list = [\r\n",
        "    'Secret Product',\r\n",
        "    'Secret Sauce'\r\n",
        "]\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "159",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-25T22:28:08.6937667Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-25T22:28:08.8136895Z",
              "execution_finish_time": "2023-07-25T22:28:08.9766384Z",
              "spark_jobs": null,
              "parent_msg_id": "e4ec970c-d994-472f-9507-ff0afca0cf12"
            },
            "text/plain": "StatementMeta(onasynapsepool, 159, 24, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "\r\n",
        "from pyspark.sql.functions import coalesce, col, count, explode, lit, md5, size, udf, max, countDistinct, dense_rank, monotonically_increasing_id\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.sql.functions import array, when\r\n",
        "from pyspark.sql.types import ArrayType, StringType, StructField, StructType, IntegerType, FloatType, DoubleType, LongType\r\n",
        "from pyspark.sql import types as t\r\n",
        "from pyspark.sql import SparkSession, Row\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "\r\n",
        "from requests import Request\r\n",
        "\r\n",
        "from synapse.ml.cognitive import *\r\n",
        "from synapse.ml import *\r\n",
        "from synapse.ml.core.platform import *\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "import re\r\n",
        "import os\r\n",
        "import typing"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "159",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-25T22:28:08.768921Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-25T22:28:09.1159739Z",
              "execution_finish_time": "2023-07-25T22:28:09.2878734Z",
              "spark_jobs": null,
              "parent_msg_id": "a5e9c8b8-7f52-42a3-8c89-26a407ae5de0"
            },
            "text/plain": "StatementMeta(onasynapsepool, 159, 25, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONNECTION TO Azure Cognitive Services\r\n",
        "\r\n",
        "# A general Cognitive Services key for Text Analytics\r\n",
        "key = \"<your_acs_key>\"\r\n",
        "loc = \"<your_acs_location>\"\r\n",
        "endpoint = \"<your_acs_endpoint>\"\r\n",
        "\r\n",
        "# Authenticate the client using key and endpoint \r\n",
        "def authenticate_client():\r\n",
        "    ta_credential = AzureKeyCredential(key)\r\n",
        "    text_analytics_client = TextAnalyticsClient(\r\n",
        "            endpoint=endpoint, \r\n",
        "            credential=ta_credential)\r\n",
        "    return text_analytics_client\r\n",
        "\r\n",
        "client = authenticate_client()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "159",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-25T22:28:08.8852317Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-25T22:28:09.4256606Z",
              "execution_finish_time": "2023-07-25T22:28:09.5764389Z",
              "spark_jobs": null,
              "parent_msg_id": "555b51ae-da58-4591-92b8-54a84e9befb1"
            },
            "text/plain": "StatementMeta(onasynapsepool, 159, 26, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA\r\n",
        "\r\n",
        "areEmailsLoaded = False\r\n",
        "areTeamsChatsLoaded = False\r\n",
        "areEnronEmailsLoaded = False\r\n",
        "areTwitterPostsLoaded = False\r\n",
        "\r\n",
        "# EMAILS\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        emailsRaw = spark.read.parquet(emailPath).select(\"Id\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        emailsRaw = spark.read.json(emailPath)\r\n",
        "        \r\n",
        "        \r\n",
        "    areEmailsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Emails data not loaded, continuing with empty emails\")\r\n",
        "    emailsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    emailsRaw = spark.createDataFrame(sc.emptyRDD(), emailsSchema)\r\n",
        "\r\n",
        "\r\n",
        "# TEAMS CHATS\r\n",
        "try:\r\n",
        "    if isParquetInput == True:\r\n",
        "        teamschatsRaw = spark.read.parquet(teamsChatPath).select(\"Id\", \"Sender\", \"ToRecipients\")\r\n",
        "    else:\r\n",
        "        teamschatsRaw = spark.read.json(teamsChatPath)\r\n",
        "    areTeamsChatsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"TeamsChats data not loaded, continuing with empty teams chats\")\r\n",
        "    teamschatsSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"EmailAddress\",StructType([StructField(\"Address\",StringType(),True),StructField(\"Name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    teamschatsRaw = spark.createDataFrame(sc.emptyRDD(), teamschatsSchema)\r\n",
        "\r\n",
        "\r\n",
        "# ENRON EMAILS\r\n",
        "try:\r\n",
        "    enronEmailsRaw = spark.read.json(enronEmailPath)\r\n",
        "\r\n",
        "    areEnronEmailsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Enron emails data not loaded, continuing with empty enron emails\")\r\n",
        "    enronSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    enronEmailsRaw = spark.createDataFrame(sc.emptyRDD(), enronSchema)\r\n",
        "\r\n",
        "\r\n",
        "# TWITTER POSTS\r\n",
        "try:\r\n",
        "    twitterPostsRaw = spark.read.json(twitterPostPath)\r\n",
        "        \r\n",
        "    areTwitterPostsLoaded = True\r\n",
        "except (Exception) as error:\r\n",
        "    print(error)\r\n",
        "    print(\"Twitter post data not loaded, continuing with empty twitter posts\")\r\n",
        "    twitterSchema = StructType([StructField(\"Id\",StringType(),True),StructField(\"Sender\",StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),StructField(\"ToRecipients\",ArrayType(StructType([StructField(\"emailAddress\",StructType([StructField(\"address\",StringType(),True),StructField(\"name\",StringType(),True)]),True)]),True),True)])\r\n",
        "    twitterPostsRaw = spark.createDataFrame(sc.emptyRDD(), twitterSchema)\r\n",
        "\r\n",
        "\r\n",
        "if (not(areEmailsLoaded) and not(areTeamsChatsLoaded)):\r\n",
        "    raise Exception(\"No Emails or TeamsChats data loaded, unable to continue. Check the file paths.\")\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "onasynapsepool",
              "session_id": "159",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "cancelled",
              "queued_time": "2023-07-25T22:28:09.0098725Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-25T22:28:09.7303971Z",
              "execution_finish_time": "2023-07-25T22:28:12.3882008Z",
              "spark_jobs": null,
              "parent_msg_id": "0f590d55-aabd-4012-81d6-353430ef0ccf"
            },
            "text/plain": "StatementMeta(onasynapsepool, 159, 27, Finished, Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMAIL CLEANING AND PREPPING\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.select(\"createdDateTime\", \"sender\", \"uniqueBody\")\r\n",
        "emailsRaw = emailsRaw.dropDuplicates()\r\n",
        "emailsRaw = emailsRaw.withColumnRenamed(\"uniqueBody\", \"body\")\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.limit(limit)\r\n",
        "\r\n",
        "emailsRaw = emailsRaw.withColumn(\"createdDateTime\", emailsRaw[\"createdDateTime\"].cast(StringType()))\r\n",
        "emailsRaw = emailsRaw.withColumn(\"sender\", emailsRaw[\"sender\"].cast(StringType()))\r\n",
        "emailsRaw = emailsRaw.withColumn(\"body\", emailsRaw[\"body\"].cast(StringType()))\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:09.1409125Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7254413Z",
              "spark_jobs": null,
              "parent_msg_id": "f3549cb9-0884-40a5-bbf5-91e2dc28fe9f"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEAMS CHATS CLEANING AND PREPPING\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.select(\"CreatedDateTime\", \"Sender\", \"Body\")\r\n",
        "teamschatsRaw = teamschatsRaw.dropDuplicates()\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"CreatedDateTime\", teamschatsRaw[\"CreatedDateTime\"].cast(StringType()))\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"Sender\", teamschatsRaw[\"Sender\"].cast(StringType()))\r\n",
        "teamschatsRaw = teamschatsRaw.withColumn(\"Body\", teamschatsRaw[\"Body\"].cast(StringType()))\r\n",
        "\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"CreatedDateTime\", \"createdDateTime\")\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"Sender\", \"sender\")\r\n",
        "teamschatsRaw = teamschatsRaw.withColumnRenamed(\"Body\", \"body\")\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:09.2884913Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7260366Z",
              "spark_jobs": null,
              "parent_msg_id": "2fd1cc04-1964-42eb-871f-ee793016d542"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMAIL PARSER (UNIQUE BODY)\r\n",
        "\r\n",
        "# given raw html email body (unique body), returns only text\r\n",
        "def parse_email(raw_email):\r\n",
        "\r\n",
        "    message = \"\"\r\n",
        "\r\n",
        "    bracket = False\r\n",
        "    for char in raw_email: \r\n",
        "        if (char == \"<\"):\r\n",
        "            bracket = False\r\n",
        "\r\n",
        "        if (bracket):\r\n",
        "            message = message + char\r\n",
        "\r\n",
        "        if (char == \">\"):\r\n",
        "            bracket = True\r\n",
        "\r\n",
        "    message_with_HTML_removed = \" \".join(word for word in message.split() if \"HTML\" not in word)\r\n",
        "\r\n",
        "    return message_with_HTML_removed\r\n",
        "\r\n",
        "\r\n",
        "# ***used in conjunction with a spark map function\r\n",
        "# given a row in an email dataframe, returns the row with the body converted to only text\r\n",
        "def parse_bodies_html_to_text(row):\r\n",
        "        raw_email = row[\"body\"]\r\n",
        "\r\n",
        "        body = parse_email(raw_email)\r\n",
        "\r\n",
        "        row_dict = row.asDict()\r\n",
        "\r\n",
        "        row_dict.update({\"body\" : body})\r\n",
        "\r\n",
        "        row = Row(**row_dict)\r\n",
        "\r\n",
        "        return row\r\n",
        "    \r\n",
        "emails = emailsRaw.rdd.map(parse_bodies_html_to_text).toDF()\r\n",
        "teamschats = teamschatsRaw.rdd.map(parse_bodies_html_to_text).toDF()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:09.4368173Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7264696Z",
              "spark_jobs": null,
              "parent_msg_id": "49ebbc52-d8fc-4906-b00c-bc0d042c7967"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MERGE EMAIL AND TEAMS CHATS DATAFRAMES\r\n",
        "\r\n",
        "emails_and_teamschats = emails.union(teamschats)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:09.6007306Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.726899Z",
              "spark_jobs": null,
              "parent_msg_id": "8e407dda-fbf4-4b6a-93ca-fac6db4ee6bd"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TWITTER POST CLEANING AND PREPPING\r\n",
        "\r\n",
        "if (areTwitterPostsLoaded):\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw.select(\"time\", \"sender\", \"body\", \"sentiment\")\r\n",
        "    twitterPostsRaw = twitterPostsRaw.select(\"time\", \"sender\", \"body\")\r\n",
        "\r\n",
        "    twitterPostsRaw = twitterPostsRaw.withColumnRenamed(\"time\", \"createdDateTime\")\r\n",
        "\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumnRenamed(\"sentiment\", \"expected_sentiment\")\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumnRenamed(\"time\", \"createdDateTime\")\r\n",
        "\r\n",
        "    twitterPostsRaw = twitterPostsRaw.limit(limit)\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.limit(limit)\r\n",
        "\r\n",
        "    twitterPostsRaw = twitterPostsRaw.withColumn(\"createdDateTime\", twitterPostsRaw[\"createdDateTime\"].cast(StringType()))\r\n",
        "    twitterPostsRaw = twitterPostsRaw.withColumn(\"sender\", twitterPostsRaw[\"sender\"].cast(StringType()))\r\n",
        "    twitterPostsRaw = twitterPostsRaw.withColumn(\"body\", twitterPostsRaw[\"body\"].cast(StringType()))\r\n",
        "\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumn(\"createdDateTime\", twitterPostsRaw_SentimentIncluded[\"createdDateTime\"].cast(StringType()))\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumn(\"sender\", twitterPostsRaw_SentimentIncluded[\"sender\"].cast(StringType()))\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumn(\"body\", twitterPostsRaw_SentimentIncluded[\"body\"].cast(StringType()))\r\n",
        "    twitterPostsRaw_SentimentIncluded = twitterPostsRaw_SentimentIncluded.withColumn(\"expected_sentiment\", twitterPostsRaw_SentimentIncluded[\"expected_sentiment\"].cast(StringType()))\r\n",
        "\r\n",
        "    twitterPosts = twitterPostsRaw\r\n",
        "    twitterPosts_SentimentIncluded = twitterPostsRaw_SentimentIncluded"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:09.8178903Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7274357Z",
              "spark_jobs": null,
              "parent_msg_id": "f18ee388-b387-40c7-a6f0-d40a39829dc2"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENRON EMAIL CLEANING AND PREPPING\r\n",
        "\r\n",
        "if (areEnronEmailsLoaded):\r\n",
        "    enronEmailsRaw = enronEmailsRaw.select(\"createdDateTime\", \"sender\", \"body\")\r\n",
        "\r\n",
        "    enronEmailsRaw = enronEmailsRaw.dropDuplicates()\r\n",
        "\r\n",
        "    enronEmailsRaw = enronEmailsRaw.limit(limit)\r\n",
        "\r\n",
        "    enronEmailsRaw = enronEmailsRaw.withColumn(\"createdDateTime\", enronEmailsRaw[\"createdDateTime\"].cast(StringType()))\r\n",
        "    enronEmailsRaw = enronEmailsRaw.withColumn(\"sender\", enronEmailsRaw[\"sender\"].cast(StringType()))\r\n",
        "    enronEmailsRaw = enronEmailsRaw.withColumn(\"body\", enronEmailsRaw[\"body\"].cast(StringType()))\r\n",
        "\r\n",
        "    enronEmails = enronEmailsRaw"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.0566518Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.728089Z",
              "spark_jobs": null,
              "parent_msg_id": "d1ad2cd4-5f7a-44e6-a488-1eed32917855"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATE PARSER\r\n",
        "\r\n",
        "def parse_date(date_string, date_format):\r\n",
        "    try:\r\n",
        "        date_string = date_string.strip()\r\n",
        "\r\n",
        "        date_obj = datetime.strptime(date_string, date_format)\r\n",
        "    \r\n",
        "        return date_obj.strftime('%Y-%m-%d')\r\n",
        "    except ValueError:\r\n",
        "        return None"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.3451294Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7287503Z",
              "spark_jobs": null,
              "parent_msg_id": "f25edf54-684f-49cb-bf20-2cfabf4232b9"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MINE OPINIONS\r\n",
        "\r\n",
        "def mine_opinions(client, documents):\r\n",
        "\r\n",
        "    # limit all documents to the first 5000 text elements to prevent throttling\r\n",
        "    documents = [doc[:5000] for doc in documents]\r\n",
        "\r\n",
        "    sentiment_result = client.analyze_sentiment(documents, show_opinion_mining=True)\r\n",
        "    entity_result = client.recognize_entities(documents)\r\n",
        "\r\n",
        "    entity_sentiment_tuples = []\r\n",
        "\r\n",
        "    document = 0\r\n",
        "    while (document < len(documents)):\r\n",
        "\r\n",
        "        categories_to_remove = [\"Person\", \"Address\", \"PhoneNumber\", \"Email\", \"URL\", \"IP\", \"DateTime\", \"Quantity\"]\r\n",
        "\r\n",
        "        document_tuples = [(CategorizedEntity, sentiment_result[document].sentiment) for CategorizedEntity in entity_result[document].entities if CategorizedEntity.category not in categories_to_remove]\r\n",
        "\r\n",
        "        entities = []\r\n",
        "        for tup in document_tuples:\r\n",
        "            if (tup[0].confidence_score > 0.6):\r\n",
        "                entity = tup[0].text\r\n",
        "                entity = re.sub(r'[^\\w\\s]', '', entity)\r\n",
        "                entity = entity.title()\r\n",
        "                entities.append((entity, tup[1]))\r\n",
        "\r\n",
        "        # fixes an issue where entities are duplicated based on their subcategories\r\n",
        "        entities_dedup = []\r\n",
        "        [entities_dedup.append(tup) for tup in entities if tup not in entities_dedup]\r\n",
        "\r\n",
        "        # filters out any entities included on the ignore list\r\n",
        "        entities = [tup for tup in entities_dedup if tup[0] not in ignore_list]\r\n",
        "\r\n",
        "        for sentence in sentiment_result[document].sentences: # break each document into sentences\r\n",
        "\r\n",
        "            if (sentence.mined_opinions): # if sentence.mined_opinions != 0 \r\n",
        "                for mined_opinion in sentence.mined_opinions: # go through mined_opinions            \r\n",
        "                    target = mined_opinion.target \r\n",
        "\r\n",
        "                    i = 0\r\n",
        "                    while (i < len(entities)):\r\n",
        "                        if (target.text == entities[i][0]):\r\n",
        "                            entities[i] = (target.text, target.sentiment)\r\n",
        "                        i = i + 1\r\n",
        "\r\n",
        "        entity_sentiment_tuples.append(entities)\r\n",
        "\r\n",
        "        document = document + 1\r\n",
        "\r\n",
        "    return entity_sentiment_tuples"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.4619685Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7295031Z",
              "spark_jobs": null,
              "parent_msg_id": "b1e07168-2d2f-4ee9-b08f-8a479fd39141"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATAFRAME CREATION\r\n",
        "\r\n",
        "def add_entities_and_sentiment_columns_to_dataframe(df):\r\n",
        "\r\n",
        "    df = df.withColumn(\"entities_and_sentiments\", lit(\"\"))\r\n",
        "\r\n",
        "    distinct_sender = df.select(\"sender\").distinct().orderBy(\"sender\")\r\n",
        "    sender_numbers = distinct_sender.withColumn(\"sender_id\", monotonically_increasing_id())\r\n",
        "    df = df.join(sender_numbers, \"sender\")\r\n",
        "\r\n",
        "    df = df.repartition(int(limit / 5))\r\n",
        "\r\n",
        "    def gather_entities_and_sentiments(client, rows):\r\n",
        "        copy_rows = list(rows)\r\n",
        "\r\n",
        "        list_of_lists_of_entity_sentiment_tuples = mine_opinions(client, [row[\"body\"] for row in copy_rows])\r\n",
        "\r\n",
        "        updated_rows = []\r\n",
        "        i = 0\r\n",
        "        for row in copy_rows: \r\n",
        "            updated_row = Row(sender = row.sender_id, body = row.body, createdDateTime = row.createdDateTime, entities_and_sentiments = list_of_lists_of_entity_sentiment_tuples[i])\r\n",
        "            updated_rows.append(updated_row)\r\n",
        "            i = i + 1\r\n",
        "\r\n",
        "        return updated_rows\r\n",
        "\r\n",
        "\r\n",
        "    df = df.rdd.mapPartitions(lambda rows: gather_entities_and_sentiments(client, rows)).toDF()\r\n",
        "\r\n",
        "    df = df.filter(col(\"entities_and_sentiments\") != array([])) \r\n",
        "    df = df.withColumnRenamed(\"sender\", \"Sender\")\r\n",
        "    df = df.select(\"Sender\", \"createdDateTime\", explode(\"entities_and_sentiments\").alias(\"entity_and_sentiment\"))\r\n",
        "\r\n",
        "    df = df.withColumn(\"Topic\", col(\"entity_and_sentiment\")[\"_1\"])\r\n",
        "    df = df.withColumn(\"Sentiment\", col(\"entity_and_sentiment\")[\"_2\"])\r\n",
        "\r\n",
        "    df = df.drop(\"entity_and_sentiment\")\r\n",
        "\r\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.521095Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7300997Z",
              "spark_jobs": null,
              "parent_msg_id": "bdcb3dbc-fcc0-4f00-82e0-e7df4fcdc010"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT DATAFRAME\r\n",
        "\r\n",
        "def create_output_dataframe_with_esa_schematics(df):\r\n",
        "    \r\n",
        "    df = df.withColumn(\"Mentions\", lit(1))\r\n",
        "\r\n",
        "    df = df.withColumn(\"Positive\", when(col(\"sentiment\") == \"positive\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Neutral\", when(col(\"sentiment\") == \"neutral\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Mixed\", when(col(\"sentiment\") == \"mixed\", 1).otherwise(0))\r\n",
        "    df = df.withColumn(\"Negative\", when(col(\"sentiment\") == \"negative\", 1).otherwise(0))\r\n",
        "\r\n",
        "    df = df.drop(\"Sentiment\")\r\n",
        "\r\n",
        "    udf_parse_date = udf(parse_date, StringType())\r\n",
        "    df = df.withColumn(\"Date Format\", lit(dtformat))\r\n",
        "    df = df.withColumn(\"Date\", udf_parse_date(col(\"createdDateTime\"), col(\"Date Format\")))\r\n",
        "    df = df.drop(\"createdDateTime\").drop(\"Date Format\")\r\n",
        "\r\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.6166244Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.7306955Z",
              "spark_jobs": null,
              "parent_msg_id": "68bee474-c2fb-4064-8760-33fa68b186dd"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT CSV\r\n",
        "\r\n",
        "print(\"Getting input dataframe...\")\r\n",
        "df = emails_and_teamschats\r\n",
        "\r\n",
        "print(\"Finding entities and sentiments...\")\r\n",
        "df = add_entities_and_sentiment_columns_to_dataframe(df)\r\n",
        "\r\n",
        "print(\"Creating output dataframe...\")\r\n",
        "df = create_output_dataframe_with_esa_schematics(df)\r\n",
        "\r\n",
        "if outputFormat == \"csv\":\r\n",
        "    print(\"Coalescing dataframe...\")\r\n",
        "    df.coalesce(1)\r\n",
        "    print(\"Saving DataFrame as CSV...\")\r\n",
        "    df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(outputPath)\r\n",
        "\r\n",
        "elif outputFormat == \"parquet\":\r\n",
        "    print(\"Coalescing dataframe...\")\r\n",
        "    df.coalesce(1)\r\n",
        "    print(\"Saving DataFrame as Parquet...\") \r\n",
        "    df.write.option(\"header\", True).mode(\"overwrite\").parquet(outputPath)\r\n",
        "\r\n",
        "print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-07-25T22:28:10.7087577Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-07-25T22:28:11.731191Z",
              "spark_jobs": null,
              "parent_msg_id": "f181cb57-c4f4-407d-9a86-b25139338048"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}