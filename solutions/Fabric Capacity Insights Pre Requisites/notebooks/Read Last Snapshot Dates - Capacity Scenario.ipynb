{"cells":[{"cell_type":"markdown","source":["# 0. Set the default lakehouse for notebook to run from pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"30ad755d-1796-42ce-ab0c-5c4fa9593ba7"},{"cell_type":"code","source":["%%configure\n","{ \n","    \"defaultLakehouse\": { \n","        \"name\": {\n","                  \"parameterName\": \"lakehouseName\",\n","                  \"defaultValue\": \"defaultlakehousename\"\n","        }\n","    }\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"6b63dc75-ee33-41f2-80e7-7de8ddc8459c"},{"cell_type":"markdown","source":["# 1. Initialize Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"a29638ad-ca6b-467b-9ad1-1963f28d964e"},{"cell_type":"code","source":["import java.time.LocalDateTime\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.UUID\n","import java.text.SimpleDateFormat\n","import java.time.{LocalDate, LocalDateTime, Period}\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.Calendar\n","import java.sql.Timestamp\n","import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val runId  = \"00000000-0000-0000-0000-000000000000\"\n","val workspaceId =  spark.conf.get(\"trident.workspace.id\")\n","val workspaceName =  \"LakeHouseTesting\"\n","val lakehouseId = spark.conf.get(\"trident.lakehouse.id\")\n","val lakehouseName =   \"IMAXDefault\"\n","val sitesStagingTableName = \"Sites_Staging\"\n","val sitesFinalTableName = \"Sites\"\n","val filesStagingTableName = \"Files_Staging\"\n","val filesFinalTableName = \"Files\"\n","val endTime  = \"2024-11-15T00:00:00Z\"\n","spark.conf.set(\"spark.sql.caseSensitive\", true)// Welcome to your new notebook\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"23b81c9a-ad2e-436a-8f6d-45f2beaf90d2"},{"cell_type":"markdown","source":["# 2. Checking Required Final Tables exists or not"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"9a3ded46-9bad-466d-b360-c6464f4941b8"},{"cell_type":"code","source":["val lakehouse  = mssparkutils.lakehouse.get(lakehouseName)\n","val lakehouseId  = lakehouse.id\n","val workspaceName = notebookutils.runtime.context(\"currentWorkspaceName\")\n","\n","val filesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesStagingTableName}\"\n","val sitesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesStagingTableName}\"\n","val sitesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesFinalTableName}\"\n","val filesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesFinalTableName}\"\n","\n","//Need to attach a lake house before this\n","val tables = spark.catalog.listTables()\n","val siteTableCount = tables.filter(col(\"name\") === lit(sitesFinalTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n","val filesTableCount = tables.filter(col(\"name\") === lit(filesFinalTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n","val siteStagingTableCount = tables.filter(col(\"name\") === lit(sitesStagingTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n","val filesStagingTableCount = tables.filter(col(\"name\") === lit(filesStagingTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"ebebff97-e480-40d1-bbaf-a0dc3ff09946"},{"cell_type":"markdown","source":["# 3. Getting Snapshot dates from last successful extracts"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"635b3aa1-c65d-4b5c-8cca-dc4ca8783527"},{"cell_type":"code","source":["import org.apache.spark.sql.functions.{col, _}\n","import org.apache.spark.sql.types._\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val dtCurrentDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.S\")\n","val dtRequiredtDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n","var siteDataExists: Boolean = false\n","var filesDataExists: Boolean = false\n","\n","val siteSnapshotDate = {\n","    if (siteTableCount ==1) {\n","        val dfSites = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${sitesFinalTableName} \")\n","        val rowSites: Row = dfSites.select(\"SnapshotDate\").head(1)(0)\n","        if (rowSites.get(0) == null) \n","            endTime \n","        else  \n","        {\n","            siteDataExists = true\n","            println(s\"Sites data Exists: ${siteDataExists}\")\n","            LocalDateTime.parse(rowSites.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt)\n","        }\n","    }\n","    else {\n","        endTime\n","    }\n","}\n","\n","val filesSnapshotDate = {\n","    if (filesTableCount ==1) {\n","        val dffiles = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${filesFinalTableName} \")\n","        val rowfiles: Row = dffiles.select(\"SnapshotDate\").head(1)(0)\n","        if (rowfiles.get(0) == null) \n","            endTime \n","        else {\n","            filesDataExists = true\n","            println(s\"files data Exists: ${filesDataExists}\")\n","            LocalDateTime.parse(rowfiles.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt) \n","        }  \n","    }\n","    else {\n","        endTime\n","    }\n","}\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"b8ee1aaa-4fce-409f-b624-3c8055c5c134"},{"cell_type":"markdown","source":["# 4. Generate View Script for Sites"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"84ce54e4-2dbf-4940-b23d-49f8fdad0d7d"},{"cell_type":"code","source":["val sitesView: String = s\"\"\"\n","CREATE OR ALTER VIEW vw${sitesFinalTableName}   \n","AS\n","SELECT  *,[StorageQuotaFriendly] =  (case \n","                when StorageQuota < 1048576 then concat(ceiling(StorageQuota / 1024.0), ' KB')\n","                when StorageQuota < 1073741824 then concat(ceiling(StorageQuota / 1048576.0), ' MB')\n","                when StorageQuota < 1099511627776  then concat(ceiling(StorageQuota / 1073741824.0), ' GB')\n","                when StorageQuota < 1125899906842624  then concat(ceiling(StorageQuota / 1099511627776.0), ' TB')\n","                else concat(ceiling(StorageQuota / 1125899906842624.0), ' PB')\n","            end )\n","        ,[StorageUsedFriendly] =  (case \n","                when StorageUsed < 1048576 then concat(ceiling(StorageUsed / 1024.0), ' KB')\n","                when StorageUsed < 1073741824 then concat(ceiling(StorageUsed / 1048576.0), ' MB')\n","                when StorageUsed < 1099511627776  then concat(ceiling(StorageUsed / 1073741824.0), ' GB')\n","                when StorageUsed < 1125899906842624  then concat(ceiling(StorageUsed / 1099511627776.0), ' TB')\n","                else concat(ceiling(StorageUsed / 1125899906842624.0), ' PB')\n","            end )            \n","  FROM ${sitesFinalTableName}\n","\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n","println(sitesView)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"c3ac128e-e185-4345-846a-d59e53bd8c0c"},{"cell_type":"markdown","source":["# 5. Generate View Script for Files"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"0a99d604-0fe2-448c-bb65-e9f7570347c8"},{"cell_type":"code","source":["val filesView: String = s\"\"\"\n","CREATE OR ALTER VIEW vw${filesFinalTableName}         \n","    AS      \n","SELECT * FROM ${filesFinalTableName}\n","\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n","println(filesView)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"57f35d38-9f13-4d08-b93c-73b79c13f320"},{"cell_type":"markdown","source":["# 6. Generate View Script for File Aggs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"60e642ea-3187-4fea-a9f4-cfd42a844d35"},{"cell_type":"code","source":["val fileAggsView: String = s\"\"\"\n","CREATE OR ALTER VIEW vw${filesFinalTableName}_Aggs  \n","    AS      \n","SELECT * FROM ${filesFinalTableName}_Aggs \n","\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n","println(fileAggsView)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"c42fbe1f-115d-43da-b96b-1e7396f302e1"},{"cell_type":"markdown","source":["# 7. Truncate the Staging tables from previous runs if data already exists"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"ea1e5bdb-4999-47d2-93b5-4c101737d2cd"},{"cell_type":"code","source":["if (siteStagingTableCount ==1) {\n","    spark.sql(s\"DELETE FROM ${lakehouseName}.${sitesStagingTableName} \")\n","    println(s\"Staging table deleted: ${lakehouseName}.${sitesStagingTableName}\")\n","}else {\n","    println(s\"Staging table ${lakehouseName}.${sitesFinalTableName} not found\")\n","}\n","\n","\n","if (filesStagingTableCount ==1) {\n","    spark.sql(s\"DELETE FROM ${lakehouseName}.${filesStagingTableName} \")\n","    println(s\"Staging table deleted: ${lakehouseName}.${filesStagingTableName}\")\n","}else {\n","    println(s\"Staging table ${lakehouseName}.${filesStagingTableName} not found\")\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"d489c12b-ecf3-4ec1-af7e-2509246fd4e0"},{"cell_type":"markdown","source":["# 8. Return snapshot dates back to Pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"70fb943c-211e-468f-9c6c-f8319720197d"},{"cell_type":"code","source":["import mssparkutils.notebook\n","val returnData= s\"\"\"{\\\"LakehouseId\\\": \\\"${lakehouseId}\\\", \\\"SitesStagingTableName\\\": \\\"${sitesStagingTableName}\\\", \\\"SitesFinalTableName\\\": \\\"${sitesFinalTableName}\\\",  \\\"SitesSnapshotDate\\\": \\\"${siteSnapshotDate}\\\", \\\"SitesDataExists\\\": ${siteDataExists}, \\\"SitesView\\\": \\\"${sitesView}\\\",  \\\"FilesStagingTableName\\\": \\\"${filesStagingTableName}\\\", \\\"FilesFinalTableName\\\": \\\"${filesFinalTableName}\\\", \\\"FilesSnapshotDate\\\": \\\"${filesSnapshotDate}\\\", \\\"EndSnapshotDate\\\": \\\"${endTime}\\\", \\\"FilesDataExists\\\": ${filesDataExists}, \\\"FilesView\\\": \\\"${filesView}\\\", \\\"FileAggsView\\\": \\\"${fileAggsView}\\\"}\"\"\"\n","println(returnData)\n","mssparkutils.notebook.exit(returnData)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"cf9c0f98-b0d0-40f5-ac5b-8d92f86e5656"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"scala"},"microsoft":{"language":"scala","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}