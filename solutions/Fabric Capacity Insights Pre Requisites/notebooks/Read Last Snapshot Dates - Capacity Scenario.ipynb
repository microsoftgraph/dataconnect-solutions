{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ad755d-1796-42ce-ab0c-5c4fa9593ba7",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 0. Set the default lakehouse for notebook to run from pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63dc75-ee33-41f2-80e7-7de8ddc8459c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{ \n",
    "    \"defaultLakehouse\": { \n",
    "        \"name\": {\n",
    "                  \"parameterName\": \"lakehouseName\",\n",
    "                  \"defaultValue\": \"defaultlakehousename\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29638ad-ca6b-467b-9ad1-1963f28d964e",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 1. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b81c9a-ad2e-436a-8f6d-45f2beaf90d2",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import java.time.LocalDateTime\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.util.UUID\n",
    "import java.text.SimpleDateFormat\n",
    "import java.time.{LocalDate, LocalDateTime, Period}\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.util.Calendar\n",
    "import java.sql.Timestamp\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    "import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n",
    "import org.apache.spark.sql.types.{StructField, _}\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val runId  = \"00000000-0000-0000-0000-000000000000\"\n",
    "val workspaceId =  spark.conf.get(\"trident.workspace.id\")\n",
    "val workspaceName =  \"LakeHouseTesting\"\n",
    "val lakehouseId = spark.conf.get(\"trident.lakehouse.id\")\n",
    "val lakehouseName =   \"IMAXDefault\"\n",
    "val sitesStagingTableName = \"Sites_Staging\"\n",
    "val sitesFinalTableName = \"Sites\"\n",
    "val filesStagingTableName = \"Files_Staging\"\n",
    "val filesFinalTableName = \"Files\"\n",
    "val endTime  = \"2024-11-15T00:00:00Z\"\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", true)// Welcome to your new notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ded46-9bad-466d-b360-c6464f4941b8",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 2. Checking Required Final Tables exists or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebff97-e480-40d1-bbaf-a0dc3ff09946",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "val lakehouse  = mssparkutils.lakehouse.get(lakehouseName)\n",
    "val lakehouseId  = lakehouse.id\n",
    "val workspaceName = notebookutils.runtime.context(\"currentWorkspaceName\")\n",
    "\n",
    "val filesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesStagingTableName}\"\n",
    "val sitesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesStagingTableName}\"\n",
    "val sitesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesFinalTableName}\"\n",
    "val filesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesFinalTableName}\"\n",
    "\n",
    "//Need to attach a lake house before this\n",
    "val tables = spark.catalog.listTables()\n",
    "val siteTableCount = tables.filter(col(\"name\") === lit(sitesFinalTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n",
    "val filesTableCount = tables.filter(col(\"name\") === lit(filesFinalTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n",
    "val siteStagingTableCount = tables.filter(col(\"name\") === lit(sitesStagingTableName)  and array_contains(col(\"namespace\"), lakehouseName) ).count()\n",
    "val filesStagingTableCount = tables.filter(col(\"name\") === lit(filesStagingTableName) and array_contains(col(\"namespace\"), lakehouseName)).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b3aa1-c65d-4b5c-8cca-dc4ca8783527",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 3. Getting Snapshot dates from last successful extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee1aaa-4fce-409f-b624-3c8055c5c134",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, _}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val dtCurrentDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.S\")\n",
    "val dtRequiredtDateFormatt = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "var siteDataExists: Boolean = false\n",
    "var filesDataExists: Boolean = false\n",
    "\n",
    "val siteSnapshotDate = {\n",
    "    if (siteTableCount ==1) {\n",
    "        val dfSites = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${sitesFinalTableName} \")\n",
    "        val rowSites: Row = dfSites.select(\"SnapshotDate\").head(1)(0)\n",
    "        if (rowSites.get(0) == null) \n",
    "            endTime \n",
    "        else  \n",
    "        {\n",
    "            siteDataExists = true\n",
    "            println(s\"Sites data Exists: ${siteDataExists}\")\n",
    "            LocalDateTime.parse(rowSites.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt)\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        endTime\n",
    "    }\n",
    "}\n",
    "\n",
    "val filesSnapshotDate = {\n",
    "    if (filesTableCount ==1) {\n",
    "        val dffiles = spark.sql(s\"SELECT MAX(SnapshotDate) AS SnapshotDate FROM ${lakehouseName}.${filesFinalTableName} \")\n",
    "        val rowfiles: Row = dffiles.select(\"SnapshotDate\").head(1)(0)\n",
    "        if (rowfiles.get(0) == null) \n",
    "            endTime \n",
    "        else {\n",
    "            filesDataExists = true\n",
    "            println(s\"files data Exists: ${filesDataExists}\")\n",
    "            LocalDateTime.parse(rowfiles.get(0).toString(), dtCurrentDateFormatt).format(dtRequiredtDateFormatt) \n",
    "        }  \n",
    "    }\n",
    "    else {\n",
    "        endTime\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce54e4-2dbf-4940-b23d-49f8fdad0d7d",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 4. Generate View Script for Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac128e-e185-4345-846a-d59e53bd8c0c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "val sitesView: String = s\"\"\"\n",
    "CREATE OR ALTER VIEW vw${sitesFinalTableName}   \n",
    "AS\n",
    "SELECT  *,[StorageQuotaFriendly] =  (case \n",
    "                when StorageQuota < 1024 then concat(StorageQuota, ' B')\n",
    "                when StorageQuota < 1048576 then concat(ceiling(StorageQuota / 1024.0), ' KB')\n",
    "                when StorageQuota < 1073741824 then concat(ceiling(StorageQuota / 1048576.0), ' MB')\n",
    "                when StorageQuota < 1099511627776  then concat(ceiling(StorageQuota / 1073741824.0), ' GB')\n",
    "                when StorageQuota < 1125899906842624  then concat(ceiling(StorageQuota / 1099511627776.0), ' TB')\n",
    "                else concat(ceiling(StorageQuota / 1125899906842624.0), ' PB')\n",
    "            end )\n",
    "        ,[StorageUsedFriendly] =  (case \n",
    "                when StorageUsed < 1024 then concat(StorageUsed, ' B')\n",
    "                when StorageUsed < 1048576 then concat(ceiling(StorageUsed / 1024.0), ' KB')\n",
    "                when StorageUsed < 1073741824 then concat(ceiling(StorageUsed / 1048576.0), ' MB')\n",
    "                when StorageUsed < 1099511627776  then concat(ceiling(StorageUsed / 1073741824.0), ' GB')\n",
    "                when StorageUsed < 1125899906842624  then concat(ceiling(StorageUsed / 1099511627776.0), ' TB')\n",
    "                else concat(ceiling(StorageUsed / 1125899906842624.0), ' PB')\n",
    "            end )\n",
    "        ,PreviousVersionStorage =  [StorageMetrics_TotalSize]-[StorageMetrics_MetadataSize]-[StorageMetrics_TotalFileStreamSize]\n",
    "        ,CreatedTimeBands =   (case \n",
    "                when [CreatedTime] is null then 'Unknown'\n",
    "                when DATEDIFF(month, [CreatedTime], [SnapshotDate]) <=1 then 'A = Up to 1 month'\n",
    "                when DATEDIFF(month, [CreatedTime], [SnapshotDate]) <=3 then 'B = 1 month - 3 months'\n",
    "                when DATEDIFF(month, [CreatedTime], [SnapshotDate]) <=6 then 'C = 3 months - 6 months'\n",
    "                when DATEDIFF(month, [CreatedTime], [SnapshotDate]) <=12 then 'D = 6 months - 1 year'\n",
    "                when DATEDIFF(month, [CreatedTime], [SnapshotDate]) <=24 then 'E = 1 year - 2 years'\n",
    "                else 'F = Over 2 years'\n",
    "                end)\n",
    "        ,LastItemModifedTimeBands =   (case \n",
    "                when [RootWeb_LastItemModifiedDate] is null then 'Unknown'\n",
    "                when DATEDIFF(month, [RootWeb_LastItemModifiedDate], [SnapshotDate]) <=1 then 'A = Up to 1 month'\n",
    "                when DATEDIFF(month, [RootWeb_LastItemModifiedDate], [SnapshotDate]) <=3 then 'B = 1 month - 3 months'\n",
    "                when DATEDIFF(month, [RootWeb_LastItemModifiedDate], [SnapshotDate]) <=6 then 'C = 3 months - 6 months'\n",
    "                when DATEDIFF(month, [RootWeb_LastItemModifiedDate], [SnapshotDate]) <=12 then 'D = 6 months - 1 year'\n",
    "                when DATEDIFF(month, [RootWeb_LastItemModifiedDate], [SnapshotDate]) <=24 then 'E = 1 year - 2 years'\n",
    "                else 'F = Over 2 years'\n",
    "                end)\n",
    "        ,OwnerStatus = (case \n",
    "                when [Owner_Email] is null and [Owner_Name] is null  then 'Missing Email and Name'\n",
    "                when [Owner_Email] is null then 'Missing Email'\n",
    "                when [Owner_Name] is null  then 'Missing Name'\n",
    "                else 'Valid'\n",
    "                end)\n",
    "        ,SiteOwnersPresent  = (case \n",
    "                when ([Owner_Email] is not null or [Owner_Name] is not null) and  ([SecondaryContact_Email] is not null or [SecondaryContact_Name] is not null)  then 'Primary and Secondary Contact'\n",
    "                when ([Owner_Email] is not null or [Owner_Name] is not null) and  ([SecondaryContact_Email] is null and  [SecondaryContact_Name] is null) then 'Only Primary Contact'\n",
    "                when ([Owner_Email] is null and [Owner_Name] is null) and ([SecondaryContact_Email] is not null or [SecondaryContact_Name] is not null)  then 'Only Secondary Contact'    \n",
    "                else 'Nether Primary nor Secondary Contact'\n",
    "                end)\n",
    "        ,StorageQuotaRemaining = [StorageQuota] - [StorageMetrics_TotalSize]\n",
    "        ,TotalStorageSizeBands = (case \n",
    "                when StorageMetrics_TotalSize is null then null\n",
    "                when StorageMetrics_TotalSize < POWER(10, 5) then 'A = Up to 100KB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 6) then 'B = 100KB - 1MB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 7) then 'C = 1MB - 10MB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 8) then 'D = 10MB - 100MB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 9) then 'E = 100MB - 1GB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 10) then 'F = 1GB - 10GB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 11) then 'G = 10GB - 100GB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 12) then 'H = 100GB - 1TB'\n",
    "                when StorageMetrics_TotalSize < POWER(10, 13) then 'I = 1TB - 10TB'\n",
    "                else 'J = Over 10TB'\n",
    "            end )            \n",
    "        ,[StorageQuotaRemainingFormatted] =  (case \n",
    "                when ([StorageQuota] - [StorageMetrics_TotalSize]) < 1024 then concat(([StorageQuota] - [StorageMetrics_TotalSize]), ' B')\n",
    "                when ([StorageQuota] - [StorageMetrics_TotalSize]) < 1048576 then concat(ceiling(([StorageQuota] - [StorageMetrics_TotalSize]) / 1024.0), ' KB')\n",
    "                when ([StorageQuota] - [StorageMetrics_TotalSize]) < 1073741824 then concat(ceiling(([StorageQuota] - [StorageMetrics_TotalSize]) / 1048576.0), ' MB')\n",
    "                when ([StorageQuota] - [StorageMetrics_TotalSize]) < 1099511627776  then concat(ceiling(([StorageQuota] - [StorageMetrics_TotalSize]) / 1073741824.0), ' GB')\n",
    "                when ([StorageQuota] - [StorageMetrics_TotalSize]) < 1125899906842624  then concat(ceiling(([StorageQuota] - [StorageMetrics_TotalSize]) / 1099511627776.0), ' TB')\n",
    "                else concat(ceiling(([StorageQuota] - [StorageMetrics_TotalSize]) / 1125899906842624.0), ' PB')\n",
    "            end )                 \n",
    "  FROM ${sitesFinalTableName}\n",
    "\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n",
    "println(sitesView)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99d604-0fe2-448c-bb65-e9f7570347c8",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 5. Generate View Script for Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f35d38-9f13-4d08-b93c-73b79c13f320",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "val filesView: String = s\"\"\"\n",
    "CREATE OR ALTER VIEW vw${filesFinalTableName}         \n",
    "    AS      \n",
    "SELECT * FROM ${filesFinalTableName}\n",
    "\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n",
    "println(filesView)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e642ea-3187-4fea-a9f4-cfd42a844d35",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 6. Generate View Script for File Aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fbe1f-115d-43da-b96b-1e7396f302e1",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "val fileAggsView: String = s\"\"\"\n",
    "CREATE OR ALTER VIEW vw${filesFinalTableName}_Aggs  \n",
    "    AS      \n",
    "SELECT * FROM ${filesFinalTableName}_Aggs \n",
    "\"\"\".stripMargin.replaceAll(\"[\\n\\r]\",\" \")\n",
    "println(fileAggsView)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e5bdb-4999-47d2-93b5-4c101737d2cd",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 7. Truncate the Staging tables from previous runs if data already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489c12b-ecf3-4ec1-af7e-2509246fd4e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if (siteStagingTableCount ==1) {\n",
    "    spark.sql(s\"DELETE FROM ${lakehouseName}.${sitesStagingTableName} \")\n",
    "    println(s\"Staging table deleted: ${lakehouseName}.${sitesStagingTableName}\")\n",
    "}else {\n",
    "    println(s\"Staging table ${lakehouseName}.${sitesFinalTableName} not found\")\n",
    "}\n",
    "\n",
    "\n",
    "if (filesStagingTableCount ==1) {\n",
    "    spark.sql(s\"DELETE FROM ${lakehouseName}.${filesStagingTableName} \")\n",
    "    println(s\"Staging table deleted: ${lakehouseName}.${filesStagingTableName}\")\n",
    "}else {\n",
    "    println(s\"Staging table ${lakehouseName}.${filesStagingTableName} not found\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb943c-211e-468f-9c6c-f8319720197d",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 8. Return snapshot dates back to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c0f98-b0d0-40f5-ac5b-8d92f86e5656",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import mssparkutils.notebook\n",
    "val returnData= s\"\"\"{\\\"LakehouseId\\\": \\\"${lakehouseId}\\\", \\\"SitesStagingTableName\\\": \\\"${sitesStagingTableName}\\\", \\\"SitesFinalTableName\\\": \\\"${sitesFinalTableName}\\\",  \\\"SitesSnapshotDate\\\": \\\"${siteSnapshotDate}\\\", \\\"SitesDataExists\\\": ${siteDataExists}, \\\"SitesView\\\": \\\"${sitesView}\\\",  \\\"FilesStagingTableName\\\": \\\"${filesStagingTableName}\\\", \\\"FilesFinalTableName\\\": \\\"${filesFinalTableName}\\\", \\\"FilesSnapshotDate\\\": \\\"${filesSnapshotDate}\\\", \\\"EndSnapshotDate\\\": \\\"${endTime}\\\", \\\"FilesDataExists\\\": ${filesDataExists}, \\\"FilesView\\\": \\\"${filesView}\\\", \\\"FileAggsView\\\": \\\"${fileAggsView}\\\"}\"\"\"\n",
    "println(returnData)\n",
    "mssparkutils.notebook.exit(returnData)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "scala"
  },
  "microsoft": {
   "language": "scala",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
