{"cells":[{"cell_type":"markdown","source":["# 0. Set the default lakehouse for notebook to run"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"506d9ad0-1561-4656-a4a6-ce8d34633858"},{"cell_type":"code","source":["%%configure\n","{ \n","    \"defaultLakehouse\": { \n","        \"name\": {\n","                  \"parameterName\": \"lakehouseName\",\n","                  \"defaultValue\": \"defaultlakehousename\"\n","        }\n","    }\n","}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"6584ead2-8df1-4d14-b610-b151e0ad37e9"},{"cell_type":"markdown","source":["# 1. Initialize Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ab7f1ea0-3bb6-409d-859e-684890193df1"},{"cell_type":"code","source":["import java.time.LocalDateTime\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.UUID\n","import java.text.SimpleDateFormat\n","import java.time.{LocalDate, LocalDateTime, Period}\n","import java.time.format.DateTimeFormatter\n","import java.time.temporal.ChronoUnit\n","import java.util.Calendar\n","import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val runId  = \"00000000-0000-0000-0000-000000000000\"\n","val workspaceId =  spark.conf.get(\"trident.workspace.id\")\n","val workspaceName =  \"LakeHouseTesting\"\n","val lakehouseId = spark.conf.get(\"trident.lakehouse.id\")\n","val lakehouseName = spark.conf.get(\"trident.lakehouse.name\")\n","val sitesStagingTableName = \"Sites_Staging\"\n","val sitesFinalTableName = \"Sites\"\n","val filesStagingTableName = \"Files_Staging\"\n","val filesFinalTableName = \"Files\"\n","spark.conf.set(\"spark.sql.caseSensitive\", true)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"a677d4e1-9d70-4143-91b6-65baef85e601"},{"cell_type":"markdown","source":["\n","# 2. Read Sites Dataset from Staging Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"449fb903-cb8e-4fe5-93e2-97870f358081"},{"cell_type":"code","source":["val lakehouse  = mssparkutils.lakehouse.get(lakehouseName)\n","val lakehouseId  = lakehouse.id\n","val workspaceName = notebookutils.runtime.context(\"currentWorkspaceName\")\n","println(\"Started reading Sites dataset\")\n","\n","val sitesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesStagingTableName}\"\n","val dfSitesStaging = spark.read.format(\"delta\").load(sitesStagingLocation)\n","println(\"Completed reading Sites dataset\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"9801b8e1-de24-48eb-91a2-1ca1e659846e"},{"cell_type":"markdown","source":["# 3. Read Files Dataset from Staging Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"e3d2123b-7b48-4d9d-b2b5-3be47a1e6f78"},{"cell_type":"code","source":["println(\"Started reading Files dataset\")\n","\n","val filesStagingLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesStagingTableName}\"\n","val dfFilesStaging = spark.read.format(\"delta\").load(filesStagingLocation)\n","println(\"Completed reading Files dataset\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"7caee3ce-c21e-4a4b-abc8-f496a0ef0b35"},{"cell_type":"markdown","source":["# 4. Check Final Tables Exists or not "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"eeab8dde-3ee6-4bd2-9ab7-7cb0b4703513"},{"cell_type":"code","source":["import io.delta.tables.DeltaTable\n","\n","val sitesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${sitesFinalTableName}\"\n","val filesFinalLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesFinalTableName}\"\n","\n","val sitesFinalTableExists = DeltaTable.isDeltaTable(spark, sitesFinalLocation)\n","if (!sitesFinalTableExists) {\n","    println(\"Final Sites table not exists. Creating final Sites table with schema only\")\n","    dfSitesStaging.filter(\"1=2\").write.format(\"delta\").mode(\"overwrite\").save(sitesFinalLocation)\n","    println(\"Final Sites table created\")\n","}else {\n","    println(\"Final Sites table exists already\")\n","}\n","\n","\n","\n","val filesFinalTableExists = DeltaTable.isDeltaTable(spark, filesFinalLocation)\n","if (!filesFinalTableExists) {\n","    println(\"Final Files table not exists. Creating final Files table with schema only\")\n","    dfFilesStaging.filter(\"1=2\").write.format(\"delta\").mode(\"overwrite\").save(filesFinalLocation)\n","    println(\"Final Files table created\")\n","}else {\n","    println(\"Final Files table exists already\")\n","}\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"dfd80dfb-017b-42c4-aec3-9abe1ba84b44"},{"cell_type":"markdown","source":["# 5. Merge Sites Data from Staging table to Final table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"a7e5dea4-90f3-4029-af65-8cef8eca910e"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val deltaTableSource = DeltaTable.forPath(spark, sitesStagingLocation)\n","val deltaTableTarget = DeltaTable.forPath(spark, sitesFinalLocation)\n","\n","import spark.implicits._\n","val dfSource = deltaTableSource.toDF\n","\n","//Delete records that have Operation as Deleted \n","println(\"Merging Sites dataset from current staging table\")\n","deltaTableTarget\n","  .as(\"target\")\n","  .merge(\n","    dfSource.as(\"source\"),\n","    \"source.Id = target.Id\")\n","  .whenMatched(\"source.Operation = 'Deleted'\")\n","  .delete()\n","  .whenMatched(\"source.Operation != 'Deleted'\")\n","  .updateAll()\n","  .whenNotMatched(\"source.Operation != 'Deleted'\")\n","  .insertAll()\n","  .execute()\n","println(\"Merging of Sites dataset completed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"117dc88d-f2d8-42de-82b2-d61bca2bc20e"},{"cell_type":"markdown","source":["# 6. Merge Files Data from Staging table to Final table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"2f67773d-edd3-4de2-8acf-48b9a0ce52e3"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val deltaTableFilesSource = DeltaTable.forPath(spark, filesStagingLocation)\n","val deltaTableFilesTarget = DeltaTable.forPath(spark, filesFinalLocation)\n","\n","import spark.implicits._\n","val dfFilesSource = deltaTableFilesSource.toDF\n","\n","//Delete records that have Operation as Deleted \n","println(\"Merging Files dataset from current staging table\")\n","\n","//Step 1: Delete all rows based on keys\n","\n","println(\"Started: Cleaned up older files\")\n","deltaTableFilesTarget\n","  .as(\"target\")\n","  .merge(\n","    dfFilesSource.as(\"source\"),\n","    \"\"\"source.SiteId = target.SiteId and source.ItemId = target.ItemId \"\"\")\n","  .whenMatched()\n","  .delete()\n","  .execute()\n","println(\"Complted: Cleaned up older files\")\n","\n","deltaTableFilesTarget\n","  .as(\"target\")\n","  .merge(\n","    dfFilesSource.as(\"source\"),\n","    \"\"\"source.SiteId = target.SiteId and source.ItemId = target.ItemId \"\"\")\n","  .whenMatched(\"source.Operation = 'Deleted'\")\n","  .delete()\n","  .whenMatched(\"source.Operation != 'Deleted'\")\n","  .updateAll()\n","  .whenNotMatched(\"source.Operation != 'Deleted'\")\n","  .insertAll()\n","  .execute()\n","  println(\"Merging of Files dataset completed\")\n","  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"c77d8298-74b4-4807-b345-a797b9ef5892"},{"cell_type":"markdown","source":["# 7. Read Sites and Files dataset - Sample TOP 10 Rows"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"b19806ee-f1ff-49e5-a0e6-32daa089f2bd"},{"cell_type":"code","source":["var sqlQuery = s\"SELECT * FROM ${lakehouseName}.${sitesFinalTableName} order by SnapshotDate DESC LIMIT 10\"\n","val dfSitesAll = spark.sql(sqlQuery)\n","display(dfSitesAll)\n","\n","sqlQuery = s\"SELECT * FROM ${lakehouseName}.${filesFinalTableName}  order by SnapshotDate DESC LIMIT 10\"\n","val dfFilesAll = spark.sql(sqlQuery)\n","display(dfFilesAll)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"},"collapsed":false},"id":"72c368c1-a1c1-410e-8f81-c2201f87a148"},{"cell_type":"markdown","source":["# 8. Check and Create Files Agg table if not exits already"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"849b3ec1-2c82-456b-9e1e-1b13de522dd8"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val filesAggTableName = filesFinalTableName + \"_Aggs\"\n","val filesAggLocation = s\"abfss://${workspaceId}@onelake.dfs.fabric.microsoft.com/${lakehouseId}/Tables/${filesAggTableName}\"\n","\n","val filesAggsTableExists = DeltaTable.isDeltaTable(spark, filesAggLocation)\n","if (!filesAggsTableExists) {\n","    println(\"Files Agg table not exists. Creating Files Agg table without schema \")\n","\n","\n","    val fileAggsSchema =\n","    StructType(Array(\n","        StructField(\"SiteId\", StringType, nullable = true),\n","        StructField(\"Extension\", StringType, nullable = true),\n","        StructField(\"FileCount_BySite\", LongType, nullable = false),\n","        StructField(\"SizeInBytes\", LongType, nullable = true))\n","    )\n","\n","    val dfFileAggsEmpty = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], fileAggsSchema)\n","    dfFileAggsEmpty.filter(\"1=2\").write.format(\"delta\").mode(\"overwrite\").save(filesAggLocation)\n","\n","    println(\"Files Agg table created\")\n","}else {\n","    println(\"Files Agg table exists already\")\n","}\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"1b880c74-a5dc-4621-888e-ba0df8a7de89"},{"cell_type":"markdown","source":["# 9. Generate File Aggs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"230bdd72-2fa1-4cdd-957a-9678d80ba8af"},{"cell_type":"code","source":["import io.delta.tables._\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.expressions.{Window, WindowSpec}\n","import org.apache.spark.sql.functions.{coalesce, lit, sum, col, _}\n","import org.apache.spark.sql.types.{StructField, _}\n","import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n","import org.apache.spark.storage.StorageLevel\n","\n","val filesDetailSource = DeltaTable.forPath(spark, filesFinalLocation)\n","val deltaTableFileAggsTarget = DeltaTable.forPath(spark, filesAggLocation)\n","\n","//Step 1: Read Files data\n","println(\"Started: Reading Files data\")\n","import spark.implicits._\n","val dfFilesDetailSource = filesDetailSource.toDF\n","\n","//Step 2: Aggregate Files data at Site and Extension Level\n","println(\"Started: Aggregating Files data at Site\")\n","val dfFileAggs = dfFilesDetailSource.groupBy(\"SiteId\", \"Extension\").agg(count(\"SiteId\").alias(\"FileCount_BySite\"), sum(\"SizeInBytes\").alias(\"SizeInBytes\"))\n","//display(dfFileAggsEmpty)\n","\n","//Step 3: Write Files data at Site and Extension Level\n","println(\"Started: Writing Files data at Site and Extension (Memory Intesive Operation). Expect to run for longer time based on data in files table\")\n","dfFileAggs.write.format(\"delta\").mode(\"overwrite\").save(filesAggLocation)\n","println(\"Completed: Writing Files data at Site and Extension\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"4ad8b3c3-0418-4966-acd1-e06ffb194aa5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"scala"},"microsoft":{"language":"scala","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}